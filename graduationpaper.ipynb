{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d556cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:48.359667Z",
     "iopub.status.busy": "2023-06-03T01:52:48.358708Z",
     "iopub.status.idle": "2023-06-03T01:52:54.638224Z",
     "shell.execute_reply": "2023-06-03T01:52:54.637018Z"
    },
    "papermill": {
     "duration": 6.292199,
     "end_time": "2023-06-03T01:52:54.641547",
     "exception": false,
     "start_time": "2023-06-03T01:52:48.349348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "import imageio\n",
    "import math\n",
    "import numpy\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "import numpy\n",
    "from transformers import AutoTokenizer\n",
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "read_images_dir = \"/kaggle/input/dataset_images/\"  # 原始图片\n",
    "save_array_dir = \"/kaggle/input/imagevector/\"  # 生成后的向量\n",
    "save_images_dir = \"/kaggle/input/prevectorimages/\"  # 统一尺寸后的图片\n",
    "\n",
    "wordPrefix = \"/kaggle/input/extract/\"  # 每个图片的物品类别 [\"id\", \"class_name\" * 5]\n",
    "dataPrefix = \"/kaggle/input/text---/\"  # 图片对应的文本 [\"id\", \"text\", \"is_sarcasm\"]\n",
    "imagePrefix = \"/kaggle/input/imageVector/\"  # 图片的对应区域向量 id.npy\n",
    "wordsPrefix = \"/kaggle/input/words-/\"  # 词表\n",
    "imageClassDir = \"/kaggle/input/extractwords/\"  # 类名对应的编号和GLove向量\n",
    "classEmbeddingDir = \"/kaggle/input/extractwords/vector\"  # 训练完成的嵌入式向量\n",
    "textEmbeddingDir = \"/kaggle/input//words-/vector\"\n",
    "imageVectorDir = \"/kaggle/input/imagevector/\"  # 图片向量的存储目录\n",
    "modelWightsDir = \"/kaggle/input/modelwights/\"  # 模型权重\n",
    "saveModelWightsDir = \"/kaggle/working/modelwights/\"\n",
    "\n",
    "if not os.path.exists(saveModelWightsDir):\n",
    "    os.mkdir(saveModelWightsDir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3c21f",
   "metadata": {
    "papermill": {
     "duration": 0.006599,
     "end_time": "2023-06-03T01:52:54.655201",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.648602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455bc84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.670648Z",
     "iopub.status.busy": "2023-06-03T01:52:54.670324Z",
     "iopub.status.idle": "2023-06-03T01:52:54.698627Z",
     "shell.execute_reply": "2023-06-03T01:52:54.697703Z"
    },
    "papermill": {
     "duration": 0.039167,
     "end_time": "2023-06-03T01:52:54.701034",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.661867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/10 21:48\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ResNet.py\n",
    "# @Description : 这个文件是用来获得预训练模型的 downsample变量不能改为其他名字，服了\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    \"\"\" 残差块 -50\"\"\"\n",
    "    expansion = 4  # 残差块第3个卷积层的通道膨胀倍率\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, down_sample=None, use_1x1conv=False):\n",
    "        \"\"\"\n",
    "        :param in_channel:残差块输入通道数\n",
    "        :param out_channel:残差块输出通道数\n",
    "        :param stride:卷积步长\n",
    "        :param down_sample:在_make_layer函数中赋值，用于控制shortcut图片下采样 H/2 W/2\n",
    "        这里的意思是 在整个卷积层的开始时，会发生 H/2 W/2\n",
    "        :param use_1x1conv:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=1, stride=1,\n",
    "                               bias=False)  # H,W不变: in_channel -> out_channel\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=out_channel)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)  # H/2，W/2 C不变\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=out_channel)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel * self.expansion, kernel_size=1,\n",
    "                               stride=1, bias=False)  # H,W不变 C: out_channel -> 4*out_channel\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=out_channel * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = down_sample\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_res = X\n",
    "        if self.downsample is not None:\n",
    "            X_res = self.downsample(X_res)\n",
    "        output = self.relu(self.bn1(self.conv1(X)))\n",
    "        output = self.relu(self.bn2(self.conv2(output)))\n",
    "        output = self.bn3(self.conv3(output))\n",
    "        output += X_res  # 残差连接\n",
    "        return self.relu(output)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, block_num, num_classes=1000):\n",
    "        \"\"\"\n",
    "        :param block:堆叠的基本模块\n",
    "        :param block_num:基本模块堆叠个数,是一个list,对于resnet50=[3,4,6,3]\n",
    "        :param num_classes:num_classes: 全连接之后的分类特征维度\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channel = 64  # conv1的输出通道数\n",
    "        # 网络开始 224 * 224-> 112 * 112\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.in_channel, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)  # H/2,W/2。C:3->64\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # 网络开始 112 * 112-> 56 * 56\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.resnet_block(block=block, channel=64, block_num=block_num[0],\n",
    "                                        stride=1)  # H W 不变 不需要下采样\n",
    "        self.layer2 = self.resnet_block(block=block, channel=128, block_num=block_num[1],\n",
    "                                        stride=2)  # H W 减半 50 101 150 需要下采样\n",
    "        self.layer3 = self.resnet_block(block=block, channel=256, block_num=block_num[2],\n",
    "                                        stride=2)  # H W 减半 50 101 150 需要下采样\n",
    "        self.layer4 = self.resnet_block(block=block, channel=512, block_num=block_num[3],\n",
    "                                        stride=2)  # H W 减半 50 101 150 需要下采样\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # 将每张特征图大小->(1,1)，则经过池化后的输出维度=通道数\n",
    "        self.fc = nn.Linear(in_features=512 * block.expansion, out_features=num_classes)\n",
    "\n",
    "        for m in self.modules():  # 权重初始化\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def resnet_block(self, block, channel, block_num, stride=1):\n",
    "        \"\"\"\n",
    "        :param block: 堆叠的基本模块\n",
    "        :param channel:基本模块堆叠个数,是一个list,对于resnet50=[3,4,6,3]\n",
    "        :param block_num:当期stage堆叠block个数\n",
    "        :param stride: 默认卷积步长\n",
    "        :return: 生成的blocks\n",
    "        \"\"\"\n",
    "        downsample = None  # 用于控制下采样的 即减半的\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=self.in_channel, out_channels=channel * block.expansion, kernel_size=1,\n",
    "                          stride=stride, bias=False),  # out_channels决定输出通道数x4，stride决定特征图尺寸H,W/2\n",
    "                nn.BatchNorm2d(num_features=channel * block.expansion))\n",
    "\n",
    "        blocks = []\n",
    "        blocks.append(block(in_channel=self.in_channel, out_channel=channel, down_sample=downsample,\n",
    "                            stride=stride))  # 定义convi_x中的第一个残差块，只有第一个需要设置down_sample和stride\n",
    "        self.in_channel = channel * block.expansion  # 在下一次调用_make_layer函数的时候，self.in_channel已经x4\n",
    "        for _ in range(1, block_num):  # 通过循环堆叠其余残差块(堆叠了剩余的block_num-1个)\n",
    "            blocks.append(block(in_channel=self.in_channel, out_channel=channel))\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = self.max_pool(self.bn1(self.bn1(self.conv1(X))))\n",
    "\n",
    "        output = self.layer1(output)\n",
    "        output = self.layer2(output)\n",
    "        output = self.layer3(output)\n",
    "        output = self.layer4(output)\n",
    "\n",
    "        output = self.avg_pool(output)\n",
    "        output = torch.flatten(output, 1)\n",
    "        # output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fabe62c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.715527Z",
     "iopub.status.busy": "2023-06-03T01:52:54.715046Z",
     "iopub.status.idle": "2023-06-03T01:52:54.726033Z",
     "shell.execute_reply": "2023-06-03T01:52:54.725087Z"
    },
    "papermill": {
     "duration": 0.020707,
     "end_time": "2023-06-03T01:52:54.728283",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.707576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/11 15:34\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ImageVector.py\n",
    "# @Description : 获得每个图片的原始特征向量\n",
    "# 该类是通过输入的标准型图片，进行多个分region后通过resnet网络得到向量后平均，生成图片模态向量\n",
    "\n",
    "\n",
    "class ImageFeature(nn.Module):\n",
    "\n",
    "    def __init__(self, net, block_num=196, kernel_size=64, stride=32, output_size=2048, in_channel=3):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size  # 输出特征向量长度\n",
    "        self.net = net  # 网络\n",
    "        self.block_num = block_num  # 生成块数\n",
    "        self.kernel_size = kernel_size  # 块的大小\n",
    "        self.stride = stride  # 步长\n",
    "        self.in_channel = in_channel  # 输入通道数\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, in_channel = input.shape[0], input.shape[1]\n",
    "        # print(\"input_size: \", input.shape)\n",
    "        output = nn.Unfold(kernel_size=(self.kernel_size, self.kernel_size), stride=self.stride)(input)\n",
    "        # print(\"unfold_size: \", output.shape)\n",
    "        output = output.transpose(1, 2).reshape(-1, in_channel, self.kernel_size,\n",
    "                                                self.kernel_size)  # 一个图片划分为多个Region (batch_size * block_num, channel, kernel_size, kernel_size)\n",
    "\n",
    "        output = self.net.forward(output).reshape(batch_size,\n",
    "                                                  self.block_num,\n",
    "                                                  self.output_size)  # 输入resNet网络后得到 (batch_size, block_num, h, w)\n",
    "\n",
    "        # # 这里的向量平均化在tensorflow的网络里做了  # # # #\n",
    "        # h = self.output_size // 64  # 这里是加速运算效果，输出默认是2048\n",
    "        # w = 64\n",
    "        # output = output.reshape(batch_size, -1, self.kernel_size, self.kernel_size)\n",
    "        # filters = torch.ones(1, output.shape[1], 1, 1) * 1.0 / output.shape[1]  # 生成过滤器\n",
    "        # output = F.conv2d(input=output, weight=filters, groups=1)  # 卷积\n",
    "        # # # # #\n",
    "        return output  # 返回向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9374f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.743341Z",
     "iopub.status.busy": "2023-06-03T01:52:54.742592Z",
     "iopub.status.idle": "2023-06-03T01:52:54.782308Z",
     "shell.execute_reply": "2023-06-03T01:52:54.781382Z"
    },
    "papermill": {
     "duration": 0.049711,
     "end_time": "2023-06-03T01:52:54.784620",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.734909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/5 10:50\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : Function.py\n",
    "# @Description :常用的函数\n",
    "\n",
    "def try_gpu(i=0):  # @save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "def getScore(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param threshold: 阈值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # y_true = y_true.flatten()\n",
    "    # y_pred = (y_pred.flatten() > threshold).type(torch.float32)\n",
    "    auc = roc_auc_score(y_true, y_pred)  # 预测值是概率\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    y_pred = (y_pred > threshold).type(torch.float32)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pre = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return acc, pre, rec, f1, auc, loss\n",
    "\n",
    "\n",
    "def getResNet50(num_classes=1000):\n",
    "    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "    return ResNet.ResNet(block=ResNet.Residual, block_num=[3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def Load_ResNet50(num_classes=1000):\n",
    "    device = try_gpu()\n",
    "    model_weight_path = modelWightsDir + \"resnet50.pth\"\n",
    "    assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
    "    net = getResNet50(num_classes)\n",
    "    net.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "    return net\n",
    "\n",
    "\n",
    "def getResNet101(num_classes=1000):\n",
    "    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
    "    return ResNet.ResNet(ResNet.Residual, [3, 4, 23, 3], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def Load_ResNet101(num_classes=1000):\n",
    "    device = try_gpu()\n",
    "    model_weight_path = modelWightsDir + \"resnet101.pth\"\n",
    "    assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
    "    net = getResNet101(num_classes)\n",
    "    net.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "    return net\n",
    "\n",
    "\n",
    "class DataSet(data.Dataset):\n",
    "    \"\"\"\n",
    "    自定义的数据集参数,用于提取图片的特征向量\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, resize):\n",
    "        super(DataSet, self).__init__()\n",
    "        self.img_paths = glob('{:s}/*'.format(img_dir))\n",
    "        self.transform = transforms.Compose([transforms.Resize(size=(resize, resize))])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.img_paths[item]).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, self.img_paths[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "\n",
    "def ProcessPreImages(img_dir, resize, save_dir):\n",
    "    \"\"\"\n",
    "    :param img_dir:\n",
    "    :param resize: 改为需要的大小\n",
    "    :param save_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--img_dir', type=str, default=img_dir)\n",
    "    parser.add_argument('--resize', type=int, default=resize)\n",
    "    parser.add_argument('--save_dir', type=str, default=save_dir)\n",
    "    args = parser.parse_args()\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.mkdir(args.save_dir)\n",
    "    else:\n",
    "        if len(os.listdir(img_dir)) >= 1:  # 说明已经有文件了 - 默认已经处理完了图片\n",
    "            return None\n",
    "\n",
    "    dataset = DataSet(args.img_dir, args.resize)\n",
    "    print('dataset:', len(dataset))\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for i in range(len(dataset)):\n",
    "        img, path = dataset[i]\n",
    "        path = os.path.basename(path)\n",
    "        if count % 1000 == 0:\n",
    "            print('Processing: ', count, \" files\")\n",
    "        count += 1\n",
    "        if not os.path.exists(args.save_dir + \"/{:s}\".format(path[0:-4])):  # 生成transformer要求的数据集格式\n",
    "            os.mkdir(args.save_dir + \"/{:s}\".format(path[0:-4]))\n",
    "        imageio.imwrite(args.save_dir + '/{:s}/{:s}'.format(path[0:-4], path), img)\n",
    "    end = time.time()\n",
    "    print(\"finished total cost: {:.2f} min\".format((end - start) / 60))\n",
    "\n",
    "\n",
    "def getDatasetIter(img_dir, batch_size, shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    :param img_dir:\n",
    "    :param batch_size: 批量大小\n",
    "    :param shuffle: 是否随机\n",
    "    :param num_workers: 使用的线程数\n",
    "    :return: 数据集, 类别名称\n",
    "    \"\"\"\n",
    "    transform = transforms.ToTensor()\n",
    "    train_data = torchvision.datasets.ImageFolder(img_dir, transform=transform)\n",
    "    print(train_data)\n",
    "    train_iter = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle,\n",
    "                                             num_workers=num_workers)\n",
    "    return train_iter, train_data.classes\n",
    "\n",
    "\n",
    "def generateImageVecFiles(imageSize=480, inChannel=3, batchSize=4, blockNum=196, kernelSize=64, stride=32,\n",
    "                          outputSize=2048):\n",
    "    \"\"\"\n",
    "    :param imageSize: 图像统一调整为多少\n",
    "    :param inChannel: 输入通道\n",
    "    :param batchSize:\n",
    "    :param blockNum: 一个图片分为多少个区域\n",
    "    :param kernelSize: 每个区域多大\n",
    "    :param stride: 步长\n",
    "    :param outputSize: 输出的向量多少\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    net = getResNet50().to(device=try_gpu())\n",
    "\n",
    "    if not os.path.exists(save_array_dir):\n",
    "        os.mkdir(save_array_dir)\n",
    "    else:\n",
    "        if len(os.listdir(save_array_dir)) >= 1:  # 说明已经有文件了 - 默认已经得到了向量\n",
    "            return None\n",
    "    if not os.path.exists(save_images_dir):\n",
    "        os.mkdir(save_images_dir)\n",
    "\n",
    "    ProcessPreImages(read_images_dir, imageSize, save_images_dir)\n",
    "    preVectorIter, classes = getDatasetIter(save_images_dir, batch_size=batchSize, shuffle=False)\n",
    "    extractImageFeature = ImageFeature(net=net, block_num=blockNum,\n",
    "                                       kernel_size=kernelSize, stride=stride,\n",
    "                                       output_size=outputSize, in_channel=inChannel)\n",
    "\n",
    "    def saveArray(array, index):\n",
    "        array = array.unsqueeze(0).detach().numpy()\n",
    "        numpy.save(save_array_dir + classes[int(index)], array)\n",
    "\n",
    "    count = 0\n",
    "    net.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for X, y in preVectorIter:\n",
    "            end = time.time()\n",
    "            if count % (batchSize * 50) == 0:\n",
    "                print(\"have got {} image vectors, total cost:{:.2f} min\".format(count, (end - start) / 60))\n",
    "            count += batchSize\n",
    "            if os.path.exists(save_array_dir + classes[int(y[0])] + \".npy\"):\n",
    "                continue\n",
    "            batch_tensor = extractImageFeature.forward(X.type(torch.float32).cuda()).to(torch.device('cpu'))\n",
    "            torch.cuda.empty_cache()\n",
    "            [saveArray(data, index) for data, index in zip(batch_tensor, y)]  # 加速\n",
    "\n",
    "\n",
    "def modelScoresVision(writer, scoresValues, scoresNames, lrValues=None):\n",
    "    \"\"\"\n",
    "    :param lrValues:\n",
    "    :param scoresNames:\n",
    "    :param writer: Tensoboard\n",
    "    :param scoresValues:  epochs * 评估参数个数 * 数据集字典\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if lrValues is not None:\n",
    "        for batch in range(len(lrValues)):\n",
    "            writer.add_scalar(tag=\"train_lr\", scalar_value=lrValues[batch], global_step=batch)\n",
    "\n",
    "    if scoresValues is None:\n",
    "        return None\n",
    "    for epoch in range(len(scoresValues) - 1):\n",
    "        for i in range(len(scoresNames)):\n",
    "            mapDict = {\n",
    "                \"train\": scoresValues[epoch][i][0],\n",
    "                \"test\": scoresValues[epoch][i][1],\n",
    "                \"valid\": scoresValues[epoch][i][2],\n",
    "            }\n",
    "            writer.add_scalars(main_tag=scoresNames[i], tag_scalar_dict=mapDict, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8535dc92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.799031Z",
     "iopub.status.busy": "2023-06-03T01:52:54.798755Z",
     "iopub.status.idle": "2023-06-03T01:52:54.804141Z",
     "shell.execute_reply": "2023-06-03T01:52:54.803169Z"
    },
    "papermill": {
     "duration": 0.015347,
     "end_time": "2023-06-03T01:52:54.806476",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.791129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/4 19:24\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : DATASET.py\n",
    "# @Description :\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class DATASET(Enum):\n",
    "    TRAIN = \"train_text\"\n",
    "    TEST = \"test_text\"\n",
    "    VALID = \"valid_text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9a085b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.821395Z",
     "iopub.status.busy": "2023-06-03T01:52:54.821096Z",
     "iopub.status.idle": "2023-06-03T01:52:54.838656Z",
     "shell.execute_reply": "2023-06-03T01:52:54.837557Z"
    },
    "papermill": {
     "duration": 0.027842,
     "end_time": "2023-06-03T01:52:54.840870",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.813028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/24 15:02\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : LoadData.py\n",
    "# @Description : 读取各种数据\n",
    "\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, seqLen, imageClassDir, imageVectorDir, textDir, wordVocabDir, dataType=DATASET.TRAIN):\n",
    "        \"\"\"\n",
    "        :param seqLen:\n",
    "        :param imageClassDir:图片对应类的字典序列化文件\n",
    "        :param imageVectorDir:ResNet生成的文本向量的文件目录\n",
    "        :param textDir:推特数据的文本数据文件\n",
    "        :param wordVocabDir:词表的字典序列化文件\n",
    "        \"\"\"\n",
    "        self.sqLen = seqLen\n",
    "        mapDataSet = {\n",
    "            DATASET.TRAIN: \"train_text\",\n",
    "            DATASET.TEST: \"test_text\",\n",
    "            DATASET.VALID: \"valid_text\"\n",
    "        }\n",
    "        self.seqLen = seqLen\n",
    "        self.imageVectorDir = imageVectorDir\n",
    "        self.id2text = []\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(modelWightsDir + \"bert-base-cased\")\n",
    "        with open(wordVocabDir + \"vocab.py3\", 'rb') as f:\n",
    "            self.word2id = pickle.load(f)  # 词表\n",
    "        with open(imageClassDir + \"class2id.py3\", 'rb') as f:\n",
    "            self.attribute2id = pickle.load(f)  # 类表\n",
    "        with open(imageClassDir + \"image2class.py3\", 'rb') as f:\n",
    "            self.dictExtractWords = pickle.load(f)  # 类表\n",
    "        with open(textDir + mapDataSet[dataType], 'r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.id2text.append(eval(line))\n",
    "        self.id2text = numpy.array(self.id2text)\n",
    "\n",
    "    def processText(self, sqLen, source):\n",
    "        \"\"\"\n",
    "        :param sqLen:文本长度\n",
    "        :param source:字符串\n",
    "        :return:对应词表的对应SqLen长度\n",
    "        \"\"\"\n",
    "        strs = source.split(\" \")\n",
    "        if len(strs) > sqLen:\n",
    "            strs = strs[:sqLen]\n",
    "        strs = numpy.array(strs)\n",
    "        func = numpy.vectorize(lambda x: self.word2id[x] if x in self.word2id else self.word2id['<unk>'])\n",
    "        return numpy.pad(func(strs), (0, sqLen - len(strs)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.id2text[index, 0]\n",
    "        text = self.id2text[index, 1]\n",
    "        reText = torch.tensor(self.processText(self.seqLen, text))\n",
    "        retY = torch.tensor(self.id2text[index, 2].astype(numpy.float32))\n",
    "        reWords = torch.tensor(itemgetter(*self.dictExtractWords[int(id)])(self.attribute2id))\n",
    "        image = torch.tensor(numpy.load(self.imageVectorDir + id + \".npy\").squeeze())\n",
    "        encodedInput = self.tokenizer(text, return_tensors='pt', padding=\"max_length\", max_length=self.sqLen,\n",
    "                                      truncation=True)\n",
    "        input_ids, token_type_ids, attention_mask = encodedInput[\"input_ids\"], encodedInput[\n",
    "            \"token_type_ids\"], encodedInput[\"attention_mask\"]\n",
    "        return (reText, image, reWords, (input_ids, token_type_ids, attention_mask)), retY\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.id2text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b28ee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.855635Z",
     "iopub.status.busy": "2023-06-03T01:52:54.855050Z",
     "iopub.status.idle": "2023-06-03T01:52:54.867534Z",
     "shell.execute_reply": "2023-06-03T01:52:54.866436Z"
    },
    "papermill": {
     "duration": 0.022406,
     "end_time": "2023-06-03T01:52:54.869911",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.847505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/25 16:13\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ExtractFeature.py\n",
    "# @Description :图像向量部\n",
    "\n",
    "\n",
    "class ExtractFeature(nn.Module):\n",
    "    def __init__(self, embeddingDir, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.embeddingArray = torch.Tensor(\n",
    "            numpy.loadtxt(embeddingDir, delimiter=\" \", dtype=\"float32\"))  # 1001 * 300 1001为1000个类和1个unk\n",
    "        if device == \"gpu\":\n",
    "            self.embeddingArray = self.embeddingArray.to(try_gpu())\n",
    "        self.embSize = self.embeddingArray.shape[1]  # 向量后的大小\n",
    "        self.vocabSize = self.embeddingArray.shape[0]  # 类表大小\n",
    "        self.embedding = nn.Embedding(self.vocabSize, self.embSize).from_pretrained(self.embeddingArray)\n",
    "        self.linear1 = nn.Linear(self.embSize, self.embSize // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.embSize // 2, 1)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.attention = AdditiveAttention(key_size=self.embSize, query_size=self.embSize,\n",
    "                                           num_hiddens=self.embSize // 2)\n",
    "        nn.init.kaiming_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, classes = X.shape[0], X.shape[1]\n",
    "        output1 = self.embedding(X)  # batch * 5 * 200\n",
    "        # # 这里也可以用之前写的注意力机制 两个版本\n",
    "        return output1, torch.mean(self.attention.forward(queries=output1, keys=output1, values=output1),\n",
    "                                   dim=1).squeeze()\n",
    "        # output2 = self.relu(self.linear1(output1))  # batch * 5 * 100\n",
    "        # output3 = self.softmax(self.linear2(output2)).reshape(batch_size, 1, classes)  # batch * 1 * 5\n",
    "        # return output1, torch.squeeze(output3 @ output1)  # batch * 5 * 100, batch * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15bed201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.885382Z",
     "iopub.status.busy": "2023-06-03T01:52:54.884583Z",
     "iopub.status.idle": "2023-06-03T01:52:54.892840Z",
     "shell.execute_reply": "2023-06-03T01:52:54.891949Z"
    },
    "papermill": {
     "duration": 0.018565,
     "end_time": "2023-06-03T01:52:54.895205",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.876640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/25 19:20\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ImageFeature.py\n",
    "# @Description :对于图片提取出来的向量加入网络\n",
    "\n",
    "class ImageFeature(nn.Module):\n",
    "    def __init__(self, defaultFeatureSize=1024, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.defaultFeatureSize = defaultFeatureSize\n",
    "        self.linear = torch.nn.Linear(2048, self.defaultFeatureSize)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # batch_size = X.shape[0]\n",
    "        output = self.relu(self.linear(X))\n",
    "        return output, torch.mean(output, dim=1)  # batch * 196 *1024,  batch  * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8af316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.910150Z",
     "iopub.status.busy": "2023-06-03T01:52:54.909562Z",
     "iopub.status.idle": "2023-06-03T01:52:54.941905Z",
     "shell.execute_reply": "2023-06-03T01:52:54.941004Z"
    },
    "papermill": {
     "duration": 0.042387,
     "end_time": "2023-06-03T01:52:54.944118",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.901731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/25 19:33\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : TextFeature.py\n",
    "# @Description :文本特征提取\n",
    "\n",
    "\n",
    "class TextFeature_LSTM(nn.Module):\n",
    "    def __init__(self, nHidden, seqLen, guideLen, textEmbeddingDir, numLayers=1, dropout=0, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        :param nHidden: 隐藏层\n",
    "        :param seqLen:  步长\n",
    "        :param guideLen: 引导向量的维度 - 物品类别嵌入后的维度\n",
    "        :param textEmbeddingDir: 文本glove后的向量\n",
    "        :param numLayers: 网络层数 - 构建深层网络结构\n",
    "        :param dropout:\n",
    "        \"\"\"\n",
    "        super(TextFeature_LSTM, self).__init__()\n",
    "        self.nHidden = nHidden\n",
    "        self.seqLen = seqLen\n",
    "        self.numLayers = numLayers\n",
    "        self.dropout = dropout\n",
    "        self.guideLen = guideLen\n",
    "        self.embeddingArray = torch.Tensor(\n",
    "            numpy.loadtxt(textEmbeddingDir, delimiter=\" \", dtype=\"float32\"))\n",
    "        if device == \"gpu\":\n",
    "            self.embeddingArray = self.embeddingArray.to(try_gpu())\n",
    "        self.embSize = self.embeddingArray.shape[1]  # 向量后的大小\n",
    "        self.vocabSize = self.embeddingArray.shape[0]  # 类表大小\n",
    "        self.embedding = nn.Embedding(self.vocabSize, self.embSize).from_pretrained(self.embeddingArray)\n",
    "        self.layerNorm = nn.LayerNorm(self.embSize)\n",
    "        self.fwLinearH = torch.nn.Linear(guideLen, self.nHidden)\n",
    "        self.fwLinearC = torch.nn.Linear(guideLen, self.nHidden)\n",
    "        self.bwLinearH = torch.nn.Linear(guideLen, self.nHidden)\n",
    "        self.bwLinearC = torch.nn.Linear(guideLen, self.nHidden)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.biLSTM = nn.LSTM(input_size=self.embSize,\n",
    "                              hidden_size=self.nHidden,\n",
    "                              batch_first=True,\n",
    "                              num_layers=self.numLayers,\n",
    "                              dropout=self.dropout,\n",
    "                              # 没有加dropout 因为不知道如何在测试时取消 20230426 - 预测时model.eval() / model.train() 来控制 20230427\n",
    "                              bidirectional=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        # 默认方法\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "        if isinstance(m, nn.LSTM):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.xavier_normal_(param)\n",
    "\n",
    "    def forward(self, X, guideVector):\n",
    "        \"\"\"\n",
    "        :param X:\n",
    "        :param guideVector:  引导向量的 - 物品类别嵌入后\n",
    "        :return: (batch_size, 2 * nHidden)\n",
    "        \"\"\"\n",
    "        if guideVector is None:\n",
    "            guideVector = torch.zeros((len(X), self.guideLen)).cuda()\n",
    "        X = self.embedding(X)\n",
    "        X = self.layerNorm(X)\n",
    "        fw_h0 = self.relu(self.fwLinearH(guideVector))\n",
    "        fw_c0 = self.relu(self.fwLinearC(guideVector))\n",
    "        bw_h0 = self.relu(self.bwLinearH(guideVector))\n",
    "        # bw_h0 = self.relu(self.fwLinearH(guideVector)) # 这里是否使用同一感知机层初始化正向和反向的H，C，需要进一步实验 20230427\n",
    "        bw_c0 = self.relu(self.bwLinearC(guideVector))\n",
    "        # bw_c0 = self.relu(self.fwLinearC(guideVector))\n",
    "        init_h0 = torch.stack((fw_h0,) * self.numLayers + (bw_h0,) * self.numLayers,\n",
    "                              dim=0)  # 深层LSTM是初始化为(D * layer , nHidden) -> (D, layers, nHidden) 观察API得出 存疑20230427\n",
    "        init_c0 = torch.stack((fw_c0,) * self.numLayers + (bw_c0,) * self.numLayers,\n",
    "                              dim=0)  # 加入stack 后网络的感知层是否会更新？ 20230427\n",
    "        output, (_, _) = self.biLSTM(X, (init_h0, init_c0))  # output = batch_size * seqLen * (2 * hidden)\n",
    "        return output, torch.mean(output, dim=1)  # batch_size * seqLen * (2 * hidden), batch_size * (2 * hidden)\n",
    "\n",
    "\n",
    "class TextFeature_Bert(nn.Module):\n",
    "    def __init__(self, nHidden, sqLen, dropout, device=\"cpu\"):\n",
    "        super(TextFeature_Bert, self).__init__()\n",
    "        self.device = device\n",
    "        self.nHidden = nHidden\n",
    "        self.sqLen = sqLen\n",
    "        self.bert = BertModel.from_pretrained(modelWightsDir + \"bert-base-cased\")\n",
    "        self.layerNorm = nn.LayerNorm(768)  # 模型一般是768 如果是别的自己改一下\n",
    "        self.linear = nn.Linear(768, self.nHidden * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        # 默认方法\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, text):\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "        with torch.no_grad():\n",
    "            output = self.bert(input_ids=input_ids.squeeze(), token_type_ids=token_type_ids.squeeze(),\n",
    "                               attention_mask=attention_mask.squeeze())[0].detach()\n",
    "        output = self.layerNorm(output)\n",
    "        output = self.tanh(self.linear(output))\n",
    "        output = self.dropout(output)\n",
    "        return output, torch.mean(output, dim=1)\n",
    "\n",
    "\n",
    "class TextFeature(nn.Module):\n",
    "    def __init__(self, nHidden, seqLen, guideLen, textEmbeddingDir, numLayers=1, dropout=0, device=\"cpu\"):\n",
    "        super(TextFeature, self).__init__()\n",
    "        self.nHidden = nHidden\n",
    "        self.lstm = TextFeature_LSTM(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                     numLayers=numLayers,\n",
    "                                     guideLen=guideLen, dropout=dropout, device=device)\n",
    "        self.bert = TextFeature_Bert(nHidden=nHidden, sqLen=seqLen, dropout=dropout)\n",
    "        self.attentionLSTM = AdditiveAttention(query_size=nHidden * 2, key_size=nHidden * 2, dropout=dropout,\n",
    "                                               num_hiddens=nHidden)\n",
    "        self.elu = nn.ELU()\n",
    "        self.lstm.apply(self.lstm.weight_init)\n",
    "        self.bert.apply(self.bert.weight_init)\n",
    "        self.attentionLSTM.apply(self.attentionLSTM.weight_init)\n",
    "\n",
    "    def forward(self, reText, text, guideVector):\n",
    "        lstm_o, lstm_vec = self.lstm.forward(reText, guideVector)\n",
    "        bert_o, bert_vec = self.bert(text)\n",
    "        lstm_o = self.attentionLSTM.forward(lstm_o, lstm_o, lstm_o)\n",
    "        o = (lstm_o + bert_o) / 2\n",
    "        output = self.elu(o)\n",
    "        output_vec = (lstm_vec + bert_vec) / 2\n",
    "        return output, output_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6145dd51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:54.958811Z",
     "iopub.status.busy": "2023-06-03T01:52:54.958524Z",
     "iopub.status.idle": "2023-06-03T01:52:54.985108Z",
     "shell.execute_reply": "2023-06-03T01:52:54.984198Z"
    },
    "papermill": {
     "duration": 0.036913,
     "end_time": "2023-06-03T01:52:54.987642",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.950729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/4 9:58\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : Attention.py\n",
    "# @Description :各种注意力机制\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"\n",
    "     通过在最后一个轴上掩蔽元素来执行softmax操作\n",
    "    :param X: X:3D张量\n",
    "    :param valid_lens: valid_lens:1D或2D张量\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                          value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout=0):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        # 默认方法\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # 在维度扩展后，\n",
    "        # queries的形状：(batchSize，查询的个数，1，num_hidden)\n",
    "        # key的形状：(batchSize，1，“键－值”对的个数，num_hiddens)\n",
    "        # 使用广播方式进行求和\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。\n",
    "        # scores的形状：(batchSize，查询的个数，“键-值”对的个数)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # values的形状：(batchSize，“键－值”对的个数，值的维度)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.attention_weights = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        \"\"\"\n",
    "        :param valid_lens: 忽略某些键值对时用\n",
    "        :param X: (batchSize，查询的个数，d)\n",
    "        :return: batchSize * 值的维度\n",
    "        \"\"\"\n",
    "        #  queries: (batchSize，查询的个数，d)\n",
    "        #  keys: (batchSize，“键－值”对的个数，d)\n",
    "        #  values: (batchSize，“键－值”对的个数，值的维度)\n",
    "        #  valid_lens: (batchSize，查询的个数)\n",
    "        queries, keys, values = X, X, X\n",
    "        d = queries.shape[-1]\n",
    "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)  # batchSize，查询个数的个数，d\n",
    "\n",
    "\n",
    "class MultiModalAttention(nn.Module):\n",
    "    \"\"\"多模态加性注意力机制融合\"\"\"\n",
    "\n",
    "    def __init__(self, querySizes, keySize, dropout=0):\n",
    "        \"\"\"\n",
    "        QKV: query = query, key=key, value=key\n",
    "        :param querySizes: 利用多个向量进行融合-源于\n",
    "        :param keySize:\n",
    "        :param dropout:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attentions = []\n",
    "        count = 0\n",
    "        for querySize in querySizes:\n",
    "            exec(\n",
    "                \"self.addATT_{} = AdditiveAttention(query_size=querySize, key_size=keySize, num_hiddens=keySize // 2, \"\n",
    "                \"dropout=dropout)\".format(\n",
    "                    count))\n",
    "            exec(\"self.attentions.append(self.addATT_{})\".format(count))\n",
    "            count += 1\n",
    "        [attention.apply(MultiModalAttention.weight_init) for attention in self.attentions]\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, queries, key):\n",
    "        \"\"\"\n",
    "        :param queries: (query1, query2...)\n",
    "        :param key: batchSize, 键值对, values\n",
    "        :return: batchSize * 1 * 值的维度\n",
    "        \"\"\"\n",
    "\n",
    "        vector = torch.zeros(key.shape[0], 1, key.shape[-1], device=key.device)  # 这里加维为了后面stack\n",
    "        for attention, query in zip(self.attentions, queries):\n",
    "            vector += attention.forward(queries=query, keys=key, values=key)\n",
    "        return vector / len(self.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6283ba7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:55.002371Z",
     "iopub.status.busy": "2023-06-03T01:52:55.001948Z",
     "iopub.status.idle": "2023-06-03T01:52:55.026841Z",
     "shell.execute_reply": "2023-06-03T01:52:55.025748Z"
    },
    "papermill": {
     "duration": 0.034852,
     "end_time": "2023-06-03T01:52:55.029112",
     "exception": false,
     "start_time": "2023-06-03T01:52:54.994260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeleteEFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nHidden, seqLen, dropout=0, numLayers=1, classEmbeddingDir=\"..//ExtractWords/vector\",\n",
    "                 textEmbeddingDir=\"../words/vector\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.FinalMLPSize = 512\n",
    "        self.device = device\n",
    "        self.extractFeature = ExtractFeature(embeddingDir=classEmbeddingDir, device=device)  # 图像中物品类别\n",
    "        self.imageFeature = ImageFeature()  # 图像特征\n",
    "        self.imageFeature.apply(ImageFeature.weight_init)\n",
    "        self.textFeature = TextFeature(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                       numLayers=numLayers,\n",
    "                                       guideLen=self.extractFeature.embSize, dropout=dropout, device=device)\n",
    "\n",
    "        # 注意力机制以 x, y, z 指导向量计算与 key的评分，最后将其平均 这里用的是加性注意力机制，由seqToSeq翻译的注意力所启发\n",
    "        self.extractFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.extractFeature.embSize, dropout=dropout)\n",
    "        self.imageFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.imageFeature.defaultFeatureSize, dropout=dropout)\n",
    "        self.textFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.textFeature.nHidden * 2, dropout=dropout)\n",
    "\n",
    "        # 为了后面的缩放点积注意力，需要把多模态向量调整为同一维度，后加入注意力机制，减少模型复杂度\n",
    "        self.extractLinear = nn.Linear(self.extractFeature.embSize, self.FinalMLPSize)\n",
    "        self.extractRelu = nn.ReLU()\n",
    "        self.imageLinear = nn.Linear(self.imageFeature.defaultFeatureSize, self.FinalMLPSize)\n",
    "        self.imageRelu = nn.ReLU()\n",
    "        self.textLinear = nn.Linear(self.textFeature.nHidden * 2, self.FinalMLPSize)\n",
    "        self.textRelu = nn.ReLU()\n",
    "\n",
    "        self.multiAttention = DotProductAttention(dropout=dropout)\n",
    "\n",
    "        # 最后加入两层全连接层\n",
    "        self.MLP, self.FC = nn.Linear(self.FinalMLPSize, self.FinalMLPSize // 2), nn.Linear(self.FinalMLPSize // 2, 1)\n",
    "        self.mlpRelu, self.fcSigmoid = nn.ReLU(), nn.Sigmoid()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reText, images, reWords, text = X\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "\n",
    "        if self.device == \"gpu\":\n",
    "            reText, images, reWords = reText.to(try_gpu()), images.to(try_gpu()), reWords.to(\n",
    "                try_gpu())\n",
    "            input_ids, token_type_ids, attention_mask = input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda()\n",
    "\n",
    "        extractMatrix, extractGuidVec = self.extractFeature.forward(reWords)\n",
    "        imageMatrix, imageGuidVec = self.imageFeature.forward(images)\n",
    "        # 改动 -------------------------------------------------------------------->\n",
    "        extractGuidVec = torch.zeros_like(extractGuidVec)  # 这里使输入为零使早期融合无效\n",
    "        # <---------------------------------------------------------------------\n",
    "        textHMatrix, textGuidVec = self.textFeature.forward(reText, (input_ids, token_type_ids, attention_mask),\n",
    "                                                            extractGuidVec)\n",
    "        extractGuidVec, imageGuidVec, textGuidVec = extractGuidVec.unsqueeze(1), imageGuidVec.unsqueeze(\n",
    "            1), textGuidVec.unsqueeze(1)  # 升维\n",
    "        extractVec = self.extractFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), extractMatrix)\n",
    "        imageVec = self.imageFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), imageMatrix)\n",
    "        textVec = self.textFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), textHMatrix)  # QVA\n",
    "\n",
    "        extractVec, imageVec, textVec = extractVec.squeeze(1), imageVec.squeeze(1), textVec.squeeze(1)  # 降维\n",
    "\n",
    "        # 是否加入relu继续激活 未实验 20230504\n",
    "        extractVec = self.extractLinear.forward(extractVec)\n",
    "        extractVec = self.extractRelu(extractVec)\n",
    "        imageVec = self.imageLinear.forward(imageVec)\n",
    "        imageVec = self.imageRelu(imageVec)\n",
    "        textVec = self.textLinear.forward(textVec)\n",
    "        textVec = self.textRelu(textVec)\n",
    "        finalMatrix = torch.stack((extractVec, imageVec, textVec), dim=1)  # 转化为 batch * 3 * FinalMLPSize\n",
    "        finalVec = torch.mean(self.multiAttention.forward(finalMatrix), dim=1)\n",
    "        fcInput = self.mlpRelu(self.MLP(finalVec))\n",
    "        return self.fcSigmoid(self.FC(fcInput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d400453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:55.044002Z",
     "iopub.status.busy": "2023-06-03T01:52:55.043726Z",
     "iopub.status.idle": "2023-06-03T01:52:55.067795Z",
     "shell.execute_reply": "2023-06-03T01:52:55.066677Z"
    },
    "papermill": {
     "duration": 0.034988,
     "end_time": "2023-06-03T01:52:55.070656",
     "exception": false,
     "start_time": "2023-06-03T01:52:55.035668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/27 11:11\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : NNManager.py\n",
    "# @Description :总体网络搭建\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, nHidden, seqLen, dropout=0, numLayers=1, classEmbeddingDir=\"..//ExtractWords/vector\",\n",
    "                 textEmbeddingDir=\"../words/vector\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.FinalMLPSize = 512\n",
    "        self.device = device\n",
    "        self.extractFeature = ExtractFeature(embeddingDir=classEmbeddingDir, device=device)  # 图像中物品类别\n",
    "        self.imageFeature = ImageFeature()  # 图像特征\n",
    "        self.imageFeature.apply(ImageFeature.weight_init)\n",
    "        self.textFeature = TextFeature(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                       numLayers=numLayers,\n",
    "                                       guideLen=self.extractFeature.embSize, dropout=dropout, device=device)\n",
    "\n",
    "        # 注意力机制以 x, y, z 指导向量计算与 key的评分，最后将其平均 这里用的是加性注意力机制\n",
    "        self.extractFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.extractFeature.embSize, dropout=dropout)\n",
    "        self.imageFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.imageFeature.defaultFeatureSize, dropout=dropout)\n",
    "        self.textFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.textFeature.nHidden * 2, dropout=dropout)\n",
    "\n",
    "        # 为了后面的缩放点积注意力，需要把多模态向量调整为同一维度，后加入注意力机制\n",
    "        self.extractLinear = nn.Linear(self.extractFeature.embSize, self.FinalMLPSize)\n",
    "        self.extractRelu = nn.ReLU()\n",
    "        self.imageLinear = nn.Linear(self.imageFeature.defaultFeatureSize, self.FinalMLPSize)\n",
    "        self.imageRelu = nn.ReLU()\n",
    "        self.textLinear = nn.Linear(self.textFeature.nHidden * 2, self.FinalMLPSize)\n",
    "        self.textRelu = nn.ReLU()\n",
    "\n",
    "        self.multiAttention = DotProductAttention(dropout=dropout)\n",
    "\n",
    "        # 最后加入两层全连接层\n",
    "        self.MLP, self.FC = nn.Linear(self.FinalMLPSize, self.FinalMLPSize // 2), nn.Linear(self.FinalMLPSize // 2, 1)\n",
    "        self.mlpRelu, self.fcSigmoid = nn.ReLU(), nn.Sigmoid()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reText, images, reWords, text = X\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "\n",
    "        if self.device == \"gpu\":\n",
    "            reText, images, reWords = reText.to(try_gpu()), images.to(try_gpu()), reWords.to(\n",
    "                try_gpu())\n",
    "            input_ids, token_type_ids, attention_mask = input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda()\n",
    "\n",
    "        extractMatrix, extractGuidVec = self.extractFeature.forward(reWords)\n",
    "        imageMatrix, imageGuidVec = self.imageFeature.forward(images)\n",
    "        textHMatrix, textGuidVec = self.textFeature.forward(reText, (input_ids, token_type_ids, attention_mask),\n",
    "                                                            extractGuidVec)\n",
    "        extractGuidVec, imageGuidVec, textGuidVec = extractGuidVec.unsqueeze(1), imageGuidVec.unsqueeze(\n",
    "            1), textGuidVec.unsqueeze(1)  # 升维\n",
    "        extractVec = self.extractFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), extractMatrix)\n",
    "        imageVec = self.imageFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), imageMatrix)\n",
    "        textVec = self.textFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), textHMatrix)\n",
    "\n",
    "        extractVec, imageVec, textVec = extractVec.squeeze(1), imageVec.squeeze(1), textVec.squeeze(1)  # 降维\n",
    "\n",
    "        # 是否加入relu继续激活 未实验 20230504\n",
    "        extractVec = self.extractLinear.forward(extractVec)\n",
    "        extractVec = self.extractRelu(extractVec)\n",
    "        imageVec = self.imageLinear.forward(imageVec)\n",
    "        imageVec = self.imageRelu(imageVec)\n",
    "        textVec = self.textLinear.forward(textVec)\n",
    "        textVec = self.textRelu(textVec)\n",
    "        finalMatrix = torch.stack((extractVec, imageVec, textVec), dim=1)  # 转化为 batch * 3 * FinalMLPSize\n",
    "        # print(\"finalMatrix.shape\", finalMatrix.shape)\n",
    "        finalVec = torch.mean(self.multiAttention.forward(finalMatrix), dim=1)\n",
    "        # print(\"finalVec.shape\", finalVec.shape)\n",
    "        fcInput = self.mlpRelu(self.MLP(finalVec))\n",
    "        # print(self.FC.weight.grad)\n",
    "        return self.fcSigmoid(self.FC(fcInput))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2a31b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:55.085395Z",
     "iopub.status.busy": "2023-06-03T01:52:55.085088Z",
     "iopub.status.idle": "2023-06-03T01:52:55.107530Z",
     "shell.execute_reply": "2023-06-03T01:52:55.106420Z"
    },
    "papermill": {
     "duration": 0.033273,
     "end_time": "2023-06-03T01:52:55.110537",
     "exception": false,
     "start_time": "2023-06-03T01:52:55.077264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/17 15:29\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : DeleteRFNet.py\n",
    "# @Description :删除了表示融合的网络\n",
    "\n",
    "class DeleteRFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nHidden, seqLen, dropout=0, numLayers=1, classEmbeddingDir=\"..//ExtractWords/vector\",\n",
    "                 textEmbeddingDir=\"../words/vector\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.FinalMLPSize = 512\n",
    "        self.device = device\n",
    "        self.extractFeature = ExtractFeature(embeddingDir=classEmbeddingDir, device=device)  # 图像中物品类别\n",
    "        self.imageFeature = ImageFeature()  # 图像特征\n",
    "        self.imageFeature.apply(ImageFeature.weight_init)\n",
    "        self.textFeature = TextFeature(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                       numLayers=numLayers,\n",
    "                                       guideLen=self.extractFeature.embSize, dropout=dropout, device=device)\n",
    "\n",
    "        # 注意力机制以 x, y, z 指导向量计算与 key的评分，最后将其平均 这里用的是加性注意力机制，由seqToSeq翻译的注意力所启发\n",
    "        self.extractFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.extractFeature.embSize, dropout=dropout)\n",
    "        self.imageFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.imageFeature.defaultFeatureSize, dropout=dropout)\n",
    "        self.textFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.textFeature.nHidden * 2, dropout=dropout)\n",
    "\n",
    "        # 为了后面的缩放点积注意力，需要把多模态向量调整为同一维度，后加入注意力机制，减少模型复杂度\n",
    "        self.extractLinear = nn.Linear(self.extractFeature.embSize, self.FinalMLPSize)\n",
    "        self.extractRelu = nn.ReLU()\n",
    "        self.imageLinear = nn.Linear(self.imageFeature.defaultFeatureSize, self.FinalMLPSize)\n",
    "        self.imageRelu = nn.ReLU()\n",
    "        self.textLinear = nn.Linear(self.textFeature.nHidden * 2, self.FinalMLPSize)\n",
    "        self.textRelu = nn.ReLU()\n",
    "\n",
    "        self.multiAttention = DotProductAttention(dropout=dropout)\n",
    "\n",
    "        # 最后加入两层全连接层\n",
    "        self.MLP, self.FC = nn.Linear(self.FinalMLPSize, self.FinalMLPSize // 2), nn.Linear(self.FinalMLPSize // 2, 1)\n",
    "        self.mlpRelu, self.fcSigmoid = nn.ReLU(), nn.Sigmoid()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reText, images, reWords, text = X\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "\n",
    "        if self.device == \"gpu\":\n",
    "            reText, images, reWords = reText.to(try_gpu()), images.to(try_gpu()), reWords.to(\n",
    "                try_gpu())\n",
    "            input_ids, token_type_ids, attention_mask = input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda()\n",
    "\n",
    "        extractMatrix, extractGuidVec = self.extractFeature.forward(reWords)\n",
    "        imageMatrix, imageGuidVec = self.imageFeature.forward(images)\n",
    "        textHMatrix, textGuidVec = self.textFeature.forward(reText, (input_ids, token_type_ids, attention_mask),\n",
    "                                                            extractGuidVec)\n",
    "        # 改动 --------------------------------------------------------------------------------------->\n",
    "        extractVec, imageVec, textVec = extractGuidVec, imageGuidVec, textGuidVec  # 降维\n",
    "        # <----------------------------------------------------------------------------------------\n",
    "\n",
    "        # 是否加入relu继续激活 未实验 20230504\n",
    "        extractVec = self.extractLinear.forward(extractVec)\n",
    "        extractVec = self.extractRelu(extractVec)\n",
    "        imageVec = self.imageLinear.forward(imageVec)\n",
    "        imageVec = self.imageRelu(imageVec)\n",
    "        textVec = self.textLinear.forward(textVec)\n",
    "        textVec = self.textRelu(textVec)\n",
    "        finalMatrix = torch.stack((extractVec, imageVec, textVec), dim=1)  # 转化为 batch * 3 * FinalMLPSize\n",
    "        finalVec = torch.mean(self.multiAttention.forward(finalMatrix), dim=1)\n",
    "        fcInput = self.mlpRelu(self.MLP(finalVec))\n",
    "        return self.fcSigmoid(self.FC(fcInput))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3e4d56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:55.125244Z",
     "iopub.status.busy": "2023-06-03T01:52:55.124939Z",
     "iopub.status.idle": "2023-06-03T01:52:55.149030Z",
     "shell.execute_reply": "2023-06-03T01:52:55.148043Z"
    },
    "papermill": {
     "duration": 0.034254,
     "end_time": "2023-06-03T01:52:55.151282",
     "exception": false,
     "start_time": "2023-06-03T01:52:55.117028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/17 15:32\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ReplaceEFNet.py\n",
    "# @Description :利用图像引导向量替代早期融合时的类别引导向量\n",
    "\n",
    "\n",
    "class ReplaceEFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nHidden, seqLen, dropout=0, numLayers=1, classEmbeddingDir=\"..//ExtractWords/vector\",\n",
    "                 textEmbeddingDir=\"../words/vector\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.FinalMLPSize = 512\n",
    "        self.device = device\n",
    "        self.extractFeature = ExtractFeature(embeddingDir=classEmbeddingDir, device=device)  # 图像中物品类别\n",
    "        self.imageFeature = ImageFeature()  # 图像特征\n",
    "        self.imageFeature.apply(ImageFeature.weight_init)\n",
    "        self.textFeature = TextFeature(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                       numLayers=numLayers,\n",
    "                                       guideLen=self.imageFeature.defaultFeatureSize, dropout=dropout, device=device)\n",
    "\n",
    "        # 注意力机制以 x, y, z 指导向量计算与 key的评分，最后将其平均 这里用的是加性注意力机制，由seqToSeq翻译的注意力所启发\n",
    "        self.extractFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.extractFeature.embSize, dropout=dropout)\n",
    "        self.imageFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.imageFeature.defaultFeatureSize, dropout=dropout)\n",
    "        self.textFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.textFeature.nHidden * 2, dropout=dropout)\n",
    "\n",
    "        # 为了后面的缩放点积注意力，需要把多模态向量调整为同一维度，后加入注意力机制，减少模型复杂度\n",
    "        self.extractLinear = nn.Linear(self.extractFeature.embSize, self.FinalMLPSize)\n",
    "        self.extractRelu = nn.ReLU()\n",
    "        self.imageLinear = nn.Linear(self.imageFeature.defaultFeatureSize, self.FinalMLPSize)\n",
    "        self.imageRelu = nn.ReLU()\n",
    "        self.textLinear = nn.Linear(self.textFeature.nHidden * 2, self.FinalMLPSize)\n",
    "        self.textRelu = nn.ReLU()\n",
    "\n",
    "        self.multiAttention = DotProductAttention(dropout=dropout)\n",
    "\n",
    "        # 最后加入两层全连接层\n",
    "        self.MLP, self.FC = nn.Linear(self.FinalMLPSize, self.FinalMLPSize // 2), nn.Linear(self.FinalMLPSize // 2, 1)\n",
    "        self.mlpRelu, self.fcSigmoid = nn.ReLU(), nn.Sigmoid()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reText, images, reWords, text = X\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "\n",
    "        if self.device == \"gpu\":\n",
    "            reText, images, reWords = reText.to(try_gpu()), images.to(try_gpu()), reWords.to(\n",
    "                try_gpu())\n",
    "            input_ids, token_type_ids, attention_mask = input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda()\n",
    "\n",
    "        extractMatrix, extractGuidVec = self.extractFeature.forward(reWords)\n",
    "        imageMatrix, imageGuidVec = self.imageFeature.forward(images)\n",
    "        # 改动 -------------------------------------------------------------------->\n",
    "        textHMatrix, textGuidVec = self.textFeature.forward(reText, (input_ids, token_type_ids, attention_mask),\n",
    "                                                            imageGuidVec)\n",
    "        # <---------------------------------------------------------------------\n",
    "        extractGuidVec, imageGuidVec, textGuidVec = extractGuidVec.unsqueeze(1), imageGuidVec.unsqueeze(\n",
    "            1), textGuidVec.unsqueeze(1)  # 升维\n",
    "        extractVec = self.extractFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), extractMatrix)\n",
    "        imageVec = self.imageFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), imageMatrix)\n",
    "        textVec = self.textFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), textHMatrix)  # QVA\n",
    "\n",
    "        extractVec, imageVec, textVec = extractVec.squeeze(1), imageVec.squeeze(1), textVec.squeeze(1)  # 降维\n",
    "\n",
    "        # 是否加入relu继续激活 未实验 20230504\n",
    "        extractVec = self.extractLinear.forward(extractVec)\n",
    "        extractVec = self.extractRelu(extractVec)\n",
    "        imageVec = self.imageLinear.forward(imageVec)\n",
    "        imageVec = self.imageRelu(imageVec)\n",
    "        textVec = self.textLinear.forward(textVec)\n",
    "        textVec = self.textRelu(textVec)\n",
    "        finalMatrix = torch.stack((extractVec, imageVec, textVec), dim=1)  # 转化为 batch * 3 * FinalMLPSize\n",
    "        finalVec = torch.mean(self.multiAttention.forward(finalMatrix), dim=1)\n",
    "        fcInput = self.mlpRelu(self.MLP(finalVec))\n",
    "        return self.fcSigmoid(self.FC(fcInput))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1fb2df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:55.166038Z",
     "iopub.status.busy": "2023-06-03T01:52:55.165762Z",
     "iopub.status.idle": "2023-06-03T01:52:55.208991Z",
     "shell.execute_reply": "2023-06-03T01:52:55.207836Z"
    },
    "papermill": {
     "duration": 0.053558,
     "end_time": "2023-06-03T01:52:55.211399",
     "exception": false,
     "start_time": "2023-06-03T01:52:55.157841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over\n"
     ]
    }
   ],
   "source": [
    "class Main:\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.lr = 1e-4  # 学习率\n",
    "        self.nHidden = 256  # 隐藏层 - Bi-LSTM\n",
    "        self.seqLen = 80  # 步长 - Bi-LSTM\n",
    "        self.numLayers = 2  # 隐藏层层数\n",
    "        self.batchSize = 128  # 批量\n",
    "        self.maxClipping = 20  # 梯度裁剪\n",
    "        self.normType = 2  # 梯度的范式\n",
    "        self.dropout = 0.2  # DropOut层的概率 留取80%\n",
    "        self.maxEpoch = 30  # 最大迭代\n",
    "        self.displayStep = 1  # 多少轮后展示训练结果ExtractFeature.py  =1时 会记录每个人epoch 当!=1时 记录maxEpoch//displayStep\n",
    "        self.maxPatience = 15  # 能够容忍多少个epoch内都没有improvement 后期也不用了前期可调\n",
    "        self.representationScores = {}\n",
    "        self.lrRecord = []  # 记录学习率变化\n",
    "        self.scoreNames = [\"acc\", \"pre\", \"rec\", \"f1\", \"auc\", \"loss\"]\n",
    "        self.XExample = None  # 获得某一个X的样本\n",
    "        self.device = device\n",
    "        self.beforeEpoch = 0  # 可以继续训练\n",
    "        self.net = Net(self.nHidden, self.seqLen, dropout=self.dropout, classEmbeddingDir=classEmbeddingDir,\n",
    "                       textEmbeddingDir=textEmbeddingDir, device=device, numLayers=self.numLayers)\n",
    "        self.net.apply(Net.weight_init)\n",
    "        self.loss = nn.BCELoss(reduction='none')\n",
    "        self.updater = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "        # 下面两个学习率衰减用法不一样\n",
    "        # self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #     optimizer=self.updater,\n",
    "        #     mode=\"min\",  # 增加/ 减小\n",
    "        #     patience=20,  # loos/acc 不再减小（或增大）的累计次数后改变学习率；\n",
    "        #     verbose=False,  # 是否可视\n",
    "        #     min_lr=1e-7,  # 最小的学习率\n",
    "        #     cooldown=10,  # 更新后冷静期\n",
    "        #     eps=1e-3  # If the difference between new and old lr is smaller than eps, the update is ignored\n",
    "        # )  # 在发现loss不再降低或者acc不再提高之后，降低学习率，这里用于批量的，所以呢，循环论数很多， 大约是 20K / batch_size\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer=self.updater,\n",
    "            T_0=5,  # 初试周期\n",
    "            T_mult=2\n",
    "        )  # 按cos函数下降 会周期性回复上来\n",
    "\n",
    "        self.train_iter = self.loadData(DATASET.TRAIN)\n",
    "\n",
    "    def loadData(self, dataType=DATASET.TRAIN):\n",
    "        data = MyDataSet(\n",
    "            seqLen=self.seqLen,\n",
    "            imageClassDir=imageClassDir,\n",
    "            imageVectorDir=imageVectorDir,\n",
    "            textDir=dataPrefix,\n",
    "            wordVocabDir=wordsPrefix,\n",
    "            dataType=dataType\n",
    "        )\n",
    "        return DataLoader(dataset=data, batch_size=self.batchSize, shuffle=True, num_workers=8,\n",
    "                          pin_memory=True, prefetch_factor=2, persistent_workers=False)\n",
    "\n",
    "    def test(self, dataType=DATASET.TEST, num=2000):\n",
    "        if isinstance(self.net, torch.nn.Module):\n",
    "            self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            testData = self.loadData(dataType=dataType)\n",
    "            count = 0\n",
    "            yPred, yTrue = [], []\n",
    "            for X, y, in testData:\n",
    "                self.XExample = X\n",
    "                if self.device == \"gpu\":\n",
    "                    y = y.cuda()\n",
    "                y_pred = self.net(X)\n",
    "                count += y_pred.shape[0]\n",
    "                if (dataType == DATASET.TRAIN) and (count > num):\n",
    "                    break\n",
    "                yPred.append(y_pred)\n",
    "                yTrue.append(y)\n",
    "        yPred = torch.cat(yPred, dim=0)\n",
    "        yTrue = torch.cat(yTrue, dim=0)\n",
    "        return getScore(y_pred=yPred.to(torch.device(\"cpu\")), y_true=yTrue.to(torch.device(\"cpu\")))\n",
    "\n",
    "    def train_epoch(self):\n",
    "        if isinstance(self.net, torch.nn.Module):\n",
    "            self.net.train()\n",
    "        if not isinstance(self.updater, torch.optim.Optimizer):\n",
    "            raise AttributeError\n",
    "        count = 0\n",
    "        for X, y in self.train_iter:\n",
    "            count += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            if self.device == \"gpu\":\n",
    "                y = y.cuda()\n",
    "            y_hat = self.net(X)\n",
    "            self.lrRecord.append(self.updater.state_dict()['param_groups'][0]['lr'])\n",
    "            l = self.loss(y_hat.squeeze(), y.squeeze()).mean()\n",
    "            self.updater.zero_grad()\n",
    "            l.backward()\n",
    "            nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=self.maxClipping, norm_type=self.normType)\n",
    "            # self.lr_scheduler.step(l)\n",
    "            self.updater.step()\n",
    "            self.lr_scheduler.step()\n",
    "            del X, y\n",
    "            # if count == 10: # 提前中止，测试用\n",
    "            #     break\n",
    "        gc.collect()\n",
    "\n",
    "    def train(self):\n",
    "        if self.device == \"gpu\":\n",
    "            self.net.to(device=try_gpu())\n",
    "        maxF1 = 0  # 以F1score为指标\n",
    "        patience = self.maxPatience  # 当前的容忍度\n",
    "        _, testScores, validScores, validStr = None, None, None, None\n",
    "        start = time.time()\n",
    "        for epoch in range(self.beforeEpoch, self.beforeEpoch + self.maxEpoch):\n",
    "            self.train_epoch()\n",
    "            if epoch % self.displayStep == 0:\n",
    "                # acc, pre, rec, f1, auc, loss # 元组内的顺序\n",
    "                trainScores, testScores, validScores = self.test(DATASET.TRAIN), self.test(DATASET.TEST), self.test(\n",
    "                    DATASET.VALID)\n",
    "                self.representationScores[epoch // self.displayStep] = tuple(\n",
    "                    zip(trainScores, testScores, validScores))  #\n",
    "                end = time.time()\n",
    "                print(\"----epoch:\", epoch, \"total cost:{:.2f} min\".format((end - start) / 60), \"---------\")\n",
    "                print(\"train, patience={}, acc:{:.3f}, pre:{:.3f}, rec:{:.3f}, f1:{:.3f}, acu:{:.3f},\"\n",
    "                      \"loss:{:.2f}\".format(patience, *trainScores))\n",
    "                print(\"test, patience={}, acc:{:.3f}, pre:{:.3f}, rec:{:.3f}, f1:{:.3f}, acu:{:.3f},\"\n",
    "                      \"loss:{:.2f}\".format(patience, *testScores))\n",
    "                validStr = \"valid, patience={}, acc:{:.3f}, pre:{:.3f}, rec:{:.3f}, f1:{:.3f}, acu:{:.3f}, loss:{:.4f}\".format(patience, *validScores)\n",
    "                print(validStr)\n",
    "                if testScores[3] > maxF1 + 1e-3:\n",
    "                    maxF1, patience = testScores[3], self.maxPatience\n",
    "                    self.saveNet(\"bestModel\", describe=validStr)\n",
    "                else:\n",
    "                    patience -= 1\n",
    "                if patience == 0:\n",
    "                    break\n",
    "        self.saveNet(describe=validStr)\n",
    "\n",
    "\n",
    "    def saveNet(self, saveName=time.strftime(\"%Y-%m-%d\", time.localtime()), describe=\"unKnown\"):\n",
    "        \"\"\"保存网络参数\"\"\"\n",
    "        savePath = saveModelWightsDir + saveName + \"/\"\n",
    "\n",
    "        if os.path.exists(savePath):\n",
    "            shutil.rmtree(savePath)  # 如果重新运行时，切忌如果有相同的文件名时要提前保存！！！！！\n",
    "        os.makedirs(savePath, exist_ok=True)\n",
    "        if not os.path.exists(savePath + \"logs/\"):\n",
    "            os.mkdir(savePath + \"logs/\")\n",
    "        if not os.path.exists(savePath + \"runs/\"):\n",
    "            os.mkdir(savePath + \"runs/\")\n",
    "\n",
    "        torch.save(self.net.state_dict(), savePath + saveName + \".pth\")\n",
    "\n",
    "        summaryWriter = SummaryWriter(log_dir=savePath + \"runs/\")\n",
    "        modelScoresVision(summaryWriter, scoresValues=self.representationScores, scoresNames=self.scoreNames,lrValues=self.lrRecord)\n",
    "        summaryWriter.close()\n",
    "\n",
    "        runLogs = (self.representationScores, self.lrRecord)\n",
    "        with open(savePath + \"logs/\" + saveName, 'wb+') as f:\n",
    "            pickle.dump(runLogs, f)\n",
    "\n",
    "        with open(savePath + \"describe.txt\", 'w+') as f:\n",
    "            f.write(\"acc, pre, rec, f1, auc, loss\\n\")\n",
    "            f.write(describe)\n",
    "\n",
    "    def loadNet(self, loadName=time.strftime(\"%Y-%m-%d\", time.localtime()), isEval=False):\n",
    "        \"\"\"加载网络参数\"\"\"\n",
    "\n",
    "        loadPath = saveModelWightsDir + loadName + \"/\"\n",
    "\n",
    "        self.net.load_state_dict(torch.load(loadPath + loadName + \".pth\"))\n",
    "\n",
    "        with open(loadPath + \"logs/\" + loadName, 'rb') as f:\n",
    "            self.representationScores, self.lrRecord = pickle.load(f)\n",
    "        self.beforeEpoch = len(self.representationScores)\n",
    "\n",
    "        if isEval:\n",
    "            self.net.eval()  # 不启用 BatchNormalization 和 Dropout\n",
    "\n",
    "print(\"Over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee52e6c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T01:52:55.226593Z",
     "iopub.status.busy": "2023-06-03T01:52:55.225855Z",
     "iopub.status.idle": "2023-06-03T03:11:44.164184Z",
     "shell.execute_reply": "2023-06-03T03:11:44.162827Z"
    },
    "papermill": {
     "duration": 4728.949137,
     "end_time": "2023-06-03T03:11:44.167319",
     "exception": false,
     "start_time": "2023-06-03T01:52:55.218182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/modelwights/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----epoch: 0 total cost:3.22 min ---------\n",
      "train, patience=15, acc:0.869, pre:0.841, rec:0.872, f1:0.856, acu:0.942,loss:0.31\n",
      "test, patience=15, acc:0.821, pre:0.750, rec:0.825, f1:0.786, acu:0.887,loss:0.42\n",
      "valid, patience=15, acc:0.835, pre:0.771, rec:0.833, f1:0.801, acu:0.903, loss:0.3826\n",
      "----epoch: 1 total cost:6.54 min ---------\n",
      "train, patience=15, acc:0.920, pre:0.910, rec:0.912, f1:0.911, acu:0.976,loss:0.21\n",
      "test, patience=15, acc:0.880, pre:0.833, rec:0.872, f1:0.852, acu:0.927,loss:0.34\n",
      "valid, patience=15, acc:0.881, pre:0.831, rec:0.879, f1:0.855, acu:0.941, loss:0.3039\n",
      "----epoch: 2 total cost:9.74 min ---------\n",
      "train, patience=15, acc:0.945, pre:0.945, rec:0.928, f1:0.936, acu:0.989,loss:0.15\n",
      "test, patience=15, acc:0.897, pre:0.853, rec:0.897, f1:0.874, acu:0.936,loss:0.34\n",
      "valid, patience=15, acc:0.911, pre:0.876, rec:0.905, f1:0.890, acu:0.954, loss:0.2703\n",
      "----epoch: 3 total cost:12.93 min ---------\n",
      "train, patience=15, acc:0.945, pre:0.961, rec:0.910, f1:0.935, acu:0.989,loss:0.15\n",
      "test, patience=15, acc:0.897, pre:0.869, rec:0.875, f1:0.872, acu:0.936,loss:0.36\n",
      "valid, patience=15, acc:0.909, pre:0.891, rec:0.879, f1:0.885, acu:0.955, loss:0.2770\n",
      "----epoch: 4 total cost:16.06 min ---------\n",
      "train, patience=14, acc:0.947, pre:0.959, rec:0.916, f1:0.937, acu:0.991,loss:0.13\n",
      "test, patience=14, acc:0.900, pre:0.859, rec:0.896, f1:0.877, acu:0.936,loss:0.36\n",
      "valid, patience=14, acc:0.914, pre:0.886, rec:0.900, f1:0.893, acu:0.956, loss:0.2725\n",
      "----epoch: 5 total cost:19.21 min ---------\n",
      "train, patience=15, acc:0.958, pre:0.967, rec:0.941, f1:0.954, acu:0.994,loss:0.11\n",
      "test, patience=15, acc:0.905, pre:0.861, rec:0.907, f1:0.883, acu:0.938,loss:0.38\n",
      "valid, patience=15, acc:0.917, pre:0.886, rec:0.909, f1:0.898, acu:0.958, loss:0.2775\n",
      "----epoch: 6 total cost:22.31 min ---------\n",
      "train, patience=15, acc:0.946, pre:0.991, rec:0.884, f1:0.934, acu:0.996,loss:0.13\n",
      "test, patience=15, acc:0.900, pre:0.889, rec:0.855, f1:0.872, acu:0.936,loss:0.44\n",
      "valid, patience=15, acc:0.910, pre:0.919, rec:0.848, f1:0.882, acu:0.958, loss:0.3174\n",
      "----epoch: 7 total cost:25.39 min ---------\n",
      "train, patience=14, acc:0.972, pre:0.988, rec:0.944, f1:0.965, acu:0.997,loss:0.09\n",
      "test, patience=14, acc:0.904, pre:0.878, rec:0.882, f1:0.880, acu:0.936,loss:0.43\n",
      "valid, patience=14, acc:0.919, pre:0.905, rec:0.888, f1:0.897, acu:0.958, loss:0.3028\n",
      "----epoch: 8 total cost:28.51 min ---------\n",
      "train, patience=13, acc:0.963, pre:0.979, rec:0.940, f1:0.959, acu:0.995,loss:0.10\n",
      "test, patience=13, acc:0.905, pre:0.872, rec:0.894, f1:0.883, acu:0.934,loss:0.43\n",
      "valid, patience=13, acc:0.920, pre:0.898, rec:0.902, f1:0.900, acu:0.958, loss:0.2919\n",
      "----epoch: 9 total cost:31.64 min ---------\n",
      "train, patience=12, acc:0.970, pre:0.961, rec:0.970, f1:0.965, acu:0.997,loss:0.08\n",
      "test, patience=12, acc:0.905, pre:0.857, rec:0.913, f1:0.884, acu:0.935,loss:0.48\n",
      "valid, patience=12, acc:0.921, pre:0.880, rec:0.927, f1:0.903, acu:0.959, loss:0.3139\n",
      "----epoch: 10 total cost:34.76 min ---------\n",
      "train, patience=15, acc:0.954, pre:0.995, rec:0.897, f1:0.944, acu:0.998,loss:0.11\n",
      "test, patience=15, acc:0.893, pre:0.888, rec:0.837, f1:0.862, acu:0.935,loss:0.48\n",
      "valid, patience=15, acc:0.900, pre:0.916, rec:0.825, f1:0.868, acu:0.958, loss:0.3475\n",
      "----epoch: 11 total cost:37.86 min ---------\n",
      "train, patience=14, acc:0.959, pre:0.990, rec:0.916, f1:0.951, acu:0.997,loss:0.10\n",
      "test, patience=14, acc:0.906, pre:0.881, rec:0.883, f1:0.882, acu:0.936,loss:0.52\n",
      "valid, patience=14, acc:0.921, pre:0.911, rec:0.887, f1:0.899, acu:0.958, loss:0.3590\n",
      "----epoch: 12 total cost:40.94 min ---------\n",
      "train, patience=13, acc:0.960, pre:0.991, rec:0.918, f1:0.953, acu:0.998,loss:0.09\n",
      "test, patience=13, acc:0.901, pre:0.885, rec:0.862, f1:0.874, acu:0.935,loss:0.57\n",
      "valid, patience=13, acc:0.917, pre:0.915, rec:0.874, f1:0.894, acu:0.959, loss:0.3786\n",
      "----epoch: 13 total cost:44.08 min ---------\n",
      "train, patience=12, acc:0.964, pre:0.996, rec:0.922, f1:0.957, acu:0.999,loss:0.09\n",
      "test, patience=12, acc:0.897, pre:0.880, rec:0.859, f1:0.870, acu:0.935,loss:0.61\n",
      "valid, patience=12, acc:0.913, pre:0.916, rec:0.861, f1:0.888, acu:0.959, loss:0.4044\n",
      "----epoch: 14 total cost:47.33 min ---------\n",
      "train, patience=11, acc:0.968, pre:0.991, rec:0.935, f1:0.962, acu:0.999,loss:0.07\n",
      "test, patience=11, acc:0.902, pre:0.879, rec:0.873, f1:0.876, acu:0.935,loss:0.61\n",
      "valid, patience=11, acc:0.920, pre:0.911, rec:0.885, f1:0.898, acu:0.959, loss:0.4018\n",
      "----epoch: 15 total cost:50.46 min ---------\n",
      "train, patience=10, acc:0.975, pre:0.999, rec:0.945, f1:0.971, acu:1.000,loss:0.06\n",
      "test, patience=10, acc:0.904, pre:0.880, rec:0.878, f1:0.879, acu:0.935,loss:0.61\n",
      "valid, patience=10, acc:0.920, pre:0.911, rec:0.886, f1:0.899, acu:0.959, loss:0.4029\n",
      "----epoch: 16 total cost:53.61 min ---------\n",
      "train, patience=9, acc:0.952, pre:0.999, rec:0.891, f1:0.942, acu:0.999,loss:0.11\n",
      "test, patience=9, acc:0.894, pre:0.889, rec:0.838, f1:0.863, acu:0.932,loss:0.58\n",
      "valid, patience=9, acc:0.905, pre:0.924, rec:0.828, f1:0.873, acu:0.958, loss:0.3910\n",
      "----epoch: 17 total cost:56.80 min ---------\n",
      "train, patience=8, acc:0.969, pre:0.995, rec:0.934, f1:0.963, acu:0.999,loss:0.08\n",
      "test, patience=8, acc:0.900, pre:0.872, rec:0.877, f1:0.875, acu:0.933,loss:0.58\n",
      "valid, patience=8, acc:0.919, pre:0.911, rec:0.883, f1:0.897, acu:0.957, loss:0.3821\n",
      "----epoch: 18 total cost:59.89 min ---------\n",
      "train, patience=7, acc:0.979, pre:0.990, rec:0.962, f1:0.976, acu:0.999,loss:0.05\n",
      "test, patience=7, acc:0.902, pre:0.873, rec:0.881, f1:0.877, acu:0.935,loss:0.62\n",
      "valid, patience=7, acc:0.922, pre:0.904, rec:0.901, f1:0.902, acu:0.959, loss:0.4020\n",
      "----epoch: 19 total cost:63.05 min ---------\n",
      "train, patience=6, acc:0.967, pre:1.000, rec:0.923, f1:0.960, acu:1.000,loss:0.08\n",
      "test, patience=6, acc:0.895, pre:0.879, rec:0.852, f1:0.865, acu:0.935,loss:0.67\n",
      "valid, patience=6, acc:0.910, pre:0.918, rec:0.851, f1:0.883, acu:0.958, loss:0.4727\n",
      "----epoch: 20 total cost:66.15 min ---------\n",
      "train, patience=5, acc:0.959, pre:1.000, rec:0.906, f1:0.951, acu:1.000,loss:0.11\n",
      "test, patience=5, acc:0.891, pre:0.890, rec:0.828, f1:0.858, acu:0.934,loss:0.80\n",
      "valid, patience=5, acc:0.903, pre:0.927, rec:0.822, f1:0.871, acu:0.958, loss:0.5750\n",
      "----epoch: 21 total cost:69.24 min ---------\n",
      "train, patience=4, acc:0.956, pre:1.000, rec:0.898, f1:0.946, acu:1.000,loss:0.10\n",
      "test, patience=4, acc:0.889, pre:0.890, rec:0.824, f1:0.855, acu:0.934,loss:0.83\n",
      "valid, patience=4, acc:0.898, pre:0.924, rec:0.809, f1:0.863, acu:0.958, loss:0.6041\n",
      "----epoch: 22 total cost:72.30 min ---------\n",
      "train, patience=3, acc:0.980, pre:0.999, rec:0.956, f1:0.977, acu:1.000,loss:0.04\n",
      "test, patience=3, acc:0.898, pre:0.880, rec:0.860, f1:0.870, acu:0.936,loss:nan\n",
      "valid, patience=3, acc:0.914, pre:0.915, rec:0.864, f1:0.889, acu:0.960, loss:nan\n",
      "----epoch: 23 total cost:75.47 min ---------\n",
      "train, patience=2, acc:0.993, pre:0.996, rec:0.986, f1:0.991, acu:1.000,loss:nan\n",
      "test, patience=2, acc:0.903, pre:0.866, rec:0.895, f1:0.880, acu:0.935,loss:nan\n",
      "valid, patience=2, acc:0.924, pre:0.896, rec:0.914, f1:0.905, acu:0.959, loss:nan\n",
      "----epoch: 24 total cost:78.56 min ---------\n",
      "train, patience=1, acc:0.994, pre:0.999, rec:0.988, f1:0.993, acu:1.000,loss:nan\n",
      "test, patience=1, acc:0.899, pre:0.871, rec:0.876, f1:0.874, acu:0.935,loss:nan\n",
      "valid, patience=1, acc:0.923, pre:0.907, rec:0.899, f1:0.903, acu:0.959, loss:nan\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "main = Main(\"gpu\")\n",
    "main.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4750.306251,
   "end_time": "2023-06-03T03:11:48.330717",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-03T01:52:38.024466",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
