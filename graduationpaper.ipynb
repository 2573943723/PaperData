{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2560791d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:46:53.304794Z",
     "iopub.status.busy": "2023-05-25T06:46:53.304196Z",
     "iopub.status.idle": "2023-05-25T06:46:59.927134Z",
     "shell.execute_reply": "2023-05-25T06:46:59.925915Z"
    },
    "papermill": {
     "duration": 6.636723,
     "end_time": "2023-05-25T06:46:59.930257",
     "exception": false,
     "start_time": "2023-05-25T06:46:53.293534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "import imageio\n",
    "import math\n",
    "import numpy\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "import numpy\n",
    "from transformers import AutoTokenizer\n",
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "read_images_dir = \"/kaggle/input/dataset_images/\"  # 原始图片\n",
    "save_array_dir = \"/kaggle/input/imagevector2/\"  # 生成后的向量\n",
    "save_images_dir = \"/kaggle/input/prevectorimages/\"  # 统一尺寸后的图片\n",
    "\n",
    "wordPrefix = \"/kaggle/input/extract/\"  # 每个图片的物品类别 [\"id\", \"class_name\" * 5]\n",
    "dataPrefix = \"/kaggle/input/text---/\"  # 图片对应的文本 [\"id\", \"text\", \"is_sarcasm\"]\n",
    "imagePrefix = \"/kaggle/input/imageVector/\"  # 图片的对应区域向量 id.npy\n",
    "wordsPrefix = \"/kaggle/input/words-/\"  # 词表\n",
    "imageClassDir = \"/kaggle/input/extractwords/\"  # 类名对应的编号和GLove向量\n",
    "classEmbeddingDir = \"/kaggle/input/extractwords/vector\"  # 训练完成的嵌入式向量\n",
    "textEmbeddingDir = \"/kaggle/input//words-/vector\"\n",
    "imageVectorDir = \"/kaggle/input/imagevector2/imageVector2/\"  # 图片向量的存储目录\n",
    "modelWightsDir = \"/kaggle/input/modelwights/\"  # 模型权重\n",
    "saveModelWightsDir = \"/kaggle/working/modelwights/\"\n",
    "\n",
    "if not os.path.exists(saveModelWightsDir):\n",
    "    os.mkdir(saveModelWightsDir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc68898",
   "metadata": {
    "papermill": {
     "duration": 0.0063,
     "end_time": "2023-05-25T06:46:59.943513",
     "exception": false,
     "start_time": "2023-05-25T06:46:59.937213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b315acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:46:59.958927Z",
     "iopub.status.busy": "2023-05-25T06:46:59.958569Z",
     "iopub.status.idle": "2023-05-25T06:46:59.986848Z",
     "shell.execute_reply": "2023-05-25T06:46:59.985768Z"
    },
    "papermill": {
     "duration": 0.038971,
     "end_time": "2023-05-25T06:46:59.989186",
     "exception": false,
     "start_time": "2023-05-25T06:46:59.950215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/10 21:48\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ResNet.py\n",
    "# @Description : 这个文件是用来获得预训练模型的 downsample变量不能改为其他名字，服了\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    \"\"\" 残差块 -50\"\"\"\n",
    "    expansion = 4  # 残差块第3个卷积层的通道膨胀倍率\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, down_sample=None, use_1x1conv=False):\n",
    "        \"\"\"\n",
    "        :param in_channel:残差块输入通道数\n",
    "        :param out_channel:残差块输出通道数\n",
    "        :param stride:卷积步长\n",
    "        :param down_sample:在_make_layer函数中赋值，用于控制shortcut图片下采样 H/2 W/2\n",
    "        这里的意思是 在整个卷积层的开始时，会发生 H/2 W/2\n",
    "        :param use_1x1conv:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=1, stride=1,\n",
    "                               bias=False)  # H,W不变: in_channel -> out_channel\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=out_channel)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)  # H/2，W/2 C不变\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=out_channel)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel * self.expansion, kernel_size=1,\n",
    "                               stride=1, bias=False)  # H,W不变 C: out_channel -> 4*out_channel\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=out_channel * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = down_sample\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_res = X\n",
    "        if self.downsample is not None:\n",
    "            X_res = self.downsample(X_res)\n",
    "        output = self.relu(self.bn1(self.conv1(X)))\n",
    "        output = self.relu(self.bn2(self.conv2(output)))\n",
    "        output = self.bn3(self.conv3(output))\n",
    "        output += X_res  # 残差连接\n",
    "        return self.relu(output)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, block_num, num_classes=1000):\n",
    "        \"\"\"\n",
    "        :param block:堆叠的基本模块\n",
    "        :param block_num:基本模块堆叠个数,是一个list,对于resnet50=[3,4,6,3]\n",
    "        :param num_classes:num_classes: 全连接之后的分类特征维度\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channel = 64  # conv1的输出通道数\n",
    "        # 网络开始 224 * 224-> 112 * 112\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.in_channel, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)  # H/2,W/2。C:3->64\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # 网络开始 112 * 112-> 56 * 56\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.resnet_block(block=block, channel=64, block_num=block_num[0],\n",
    "                                        stride=1)  # H W 不变 不需要下采样\n",
    "        self.layer2 = self.resnet_block(block=block, channel=128, block_num=block_num[1],\n",
    "                                        stride=2)  # H W 减半 50 101 150 需要下采样\n",
    "        self.layer3 = self.resnet_block(block=block, channel=256, block_num=block_num[2],\n",
    "                                        stride=2)  # H W 减半 50 101 150 需要下采样\n",
    "        self.layer4 = self.resnet_block(block=block, channel=512, block_num=block_num[3],\n",
    "                                        stride=2)  # H W 减半 50 101 150 需要下采样\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # 将每张特征图大小->(1,1)，则经过池化后的输出维度=通道数\n",
    "        self.fc = nn.Linear(in_features=512 * block.expansion, out_features=num_classes)\n",
    "\n",
    "        for m in self.modules():  # 权重初始化\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def resnet_block(self, block, channel, block_num, stride=1):\n",
    "        \"\"\"\n",
    "        :param block: 堆叠的基本模块\n",
    "        :param channel:基本模块堆叠个数,是一个list,对于resnet50=[3,4,6,3]\n",
    "        :param block_num:当期stage堆叠block个数\n",
    "        :param stride: 默认卷积步长\n",
    "        :return: 生成的blocks\n",
    "        \"\"\"\n",
    "        downsample = None  # 用于控制下采样的 即减半的\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=self.in_channel, out_channels=channel * block.expansion, kernel_size=1,\n",
    "                          stride=stride, bias=False),  # out_channels决定输出通道数x4，stride决定特征图尺寸H,W/2\n",
    "                nn.BatchNorm2d(num_features=channel * block.expansion))\n",
    "\n",
    "        blocks = []\n",
    "        blocks.append(block(in_channel=self.in_channel, out_channel=channel, down_sample=downsample,\n",
    "                            stride=stride))  # 定义convi_x中的第一个残差块，只有第一个需要设置down_sample和stride\n",
    "        self.in_channel = channel * block.expansion  # 在下一次调用_make_layer函数的时候，self.in_channel已经x4\n",
    "        for _ in range(1, block_num):  # 通过循环堆叠其余残差块(堆叠了剩余的block_num-1个)\n",
    "            blocks.append(block(in_channel=self.in_channel, out_channel=channel))\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = self.max_pool(self.bn1(self.bn1(self.conv1(X))))\n",
    "\n",
    "        output = self.layer1(output)\n",
    "        output = self.layer2(output)\n",
    "        output = self.layer3(output)\n",
    "        output = self.layer4(output)\n",
    "\n",
    "        output = self.avg_pool(output)\n",
    "        output = torch.flatten(output, 1)\n",
    "        # output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74ce4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.003849Z",
     "iopub.status.busy": "2023-05-25T06:47:00.003536Z",
     "iopub.status.idle": "2023-05-25T06:47:00.013727Z",
     "shell.execute_reply": "2023-05-25T06:47:00.012624Z"
    },
    "papermill": {
     "duration": 0.02027,
     "end_time": "2023-05-25T06:47:00.016013",
     "exception": false,
     "start_time": "2023-05-25T06:46:59.995743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/11 15:34\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ImageVector.py\n",
    "# @Description : 获得每个图片的原始特征向量\n",
    "# 该类是通过输入的标准型图片，进行多个分region后通过resnet网络得到向量后平均，生成图片模态向量\n",
    "\n",
    "\n",
    "class ImageFeature(nn.Module):\n",
    "\n",
    "    def __init__(self, net, block_num=196, kernel_size=64, stride=32, output_size=2048, in_channel=3):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size  # 输出特征向量长度\n",
    "        self.net = net  # 网络\n",
    "        self.block_num = block_num  # 生成块数\n",
    "        self.kernel_size = kernel_size  # 块的大小\n",
    "        self.stride = stride  # 步长\n",
    "        self.in_channel = in_channel  # 输入通道数\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, in_channel = input.shape[0], input.shape[1]\n",
    "        # print(\"input_size: \", input.shape)\n",
    "        output = nn.Unfold(kernel_size=(self.kernel_size, self.kernel_size), stride=self.stride)(input)\n",
    "        # print(\"unfold_size: \", output.shape)\n",
    "        output = output.transpose(1, 2).reshape(-1, in_channel, self.kernel_size,\n",
    "                                                self.kernel_size)  # 一个图片划分为多个Region (batch_size * block_num, channel, kernel_size, kernel_size)\n",
    "\n",
    "        output = self.net.forward(output).reshape(batch_size,\n",
    "                                                  self.block_num,\n",
    "                                                  self.output_size)  # 输入resNet网络后得到 (batch_size, block_num, h, w)\n",
    "\n",
    "        # # 这里的向量平均化在tensorflow的网络里做了  # # # #\n",
    "        # h = self.output_size // 64  # 这里是加速运算效果，输出默认是2048\n",
    "        # w = 64\n",
    "        # output = output.reshape(batch_size, -1, self.kernel_size, self.kernel_size)\n",
    "        # filters = torch.ones(1, output.shape[1], 1, 1) * 1.0 / output.shape[1]  # 生成过滤器\n",
    "        # output = F.conv2d(input=output, weight=filters, groups=1)  # 卷积\n",
    "        # # # # #\n",
    "        return output  # 返回向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ad6f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.031105Z",
     "iopub.status.busy": "2023-05-25T06:47:00.030778Z",
     "iopub.status.idle": "2023-05-25T06:47:00.072125Z",
     "shell.execute_reply": "2023-05-25T06:47:00.070928Z"
    },
    "papermill": {
     "duration": 0.051892,
     "end_time": "2023-05-25T06:47:00.074548",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.022656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/5 10:50\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : Function.py\n",
    "# @Description :常用的函数\n",
    "\n",
    "def try_gpu(i=0):  # @save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "def getScore(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param threshold: 阈值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # y_true = y_true.flatten()\n",
    "    # y_pred = (y_pred.flatten() > threshold).type(torch.float32)\n",
    "    auc = roc_auc_score(y_true, y_pred)  # 预测值是概率\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    y_pred = (y_pred > threshold).type(torch.float32)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pre = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return acc, pre, rec, f1, auc, loss\n",
    "\n",
    "\n",
    "def getResNet50(num_classes=1000):\n",
    "    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "    return ResNet.ResNet(block=ResNet.Residual, block_num=[3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def Load_ResNet50(num_classes=1000):\n",
    "    device = try_gpu()\n",
    "    model_weight_path = modelWightsDir + \"resnet50.pth\"\n",
    "    assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
    "    net = getResNet50(num_classes)\n",
    "    net.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "    return net\n",
    "\n",
    "\n",
    "def getResNet101(num_classes=1000):\n",
    "    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
    "    return ResNet.ResNet(ResNet.Residual, [3, 4, 23, 3], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def Load_ResNet101(num_classes=1000):\n",
    "    device = try_gpu()\n",
    "    model_weight_path = modelWightsDir + \"resnet101.pth\"\n",
    "    assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
    "    net = getResNet101(num_classes)\n",
    "    net.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "    return net\n",
    "\n",
    "\n",
    "class DataSet(data.Dataset):\n",
    "    \"\"\"\n",
    "    自定义的数据集参数,用于提取图片的特征向量\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, resize):\n",
    "        super(DataSet, self).__init__()\n",
    "        self.img_paths = glob('{:s}/*'.format(img_dir))\n",
    "        self.transform = transforms.Compose([transforms.Resize(size=(resize, resize))])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.img_paths[item]).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, self.img_paths[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "\n",
    "def ProcessPreImages(img_dir, resize, save_dir):\n",
    "    \"\"\"\n",
    "    :param img_dir:\n",
    "    :param resize: 改为需要的大小\n",
    "    :param save_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--img_dir', type=str, default=img_dir)\n",
    "    parser.add_argument('--resize', type=int, default=resize)\n",
    "    parser.add_argument('--save_dir', type=str, default=save_dir)\n",
    "    args = parser.parse_args()\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.mkdir(args.save_dir)\n",
    "    else:\n",
    "        if len(os.listdir(img_dir)) >= 1:  # 说明已经有文件了 - 默认已经处理完了图片\n",
    "            return None\n",
    "\n",
    "    dataset = DataSet(args.img_dir, args.resize)\n",
    "    print('dataset:', len(dataset))\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for i in range(len(dataset)):\n",
    "        img, path = dataset[i]\n",
    "        path = os.path.basename(path)\n",
    "        if count % 1000 == 0:\n",
    "            print('Processing: ', count, \" files\")\n",
    "        count += 1\n",
    "        if not os.path.exists(args.save_dir + \"/{:s}\".format(path[0:-4])):  # 生成transformer要求的数据集格式\n",
    "            os.mkdir(args.save_dir + \"/{:s}\".format(path[0:-4]))\n",
    "        imageio.imwrite(args.save_dir + '/{:s}/{:s}'.format(path[0:-4], path), img)\n",
    "    end = time.time()\n",
    "    print(\"finished total cost: {:.2f} min\".format((end - start) / 60))\n",
    "\n",
    "\n",
    "def getDatasetIter(img_dir, batch_size, shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    :param img_dir:\n",
    "    :param batch_size: 批量大小\n",
    "    :param shuffle: 是否随机\n",
    "    :param num_workers: 使用的线程数\n",
    "    :return: 数据集, 类别名称\n",
    "    \"\"\"\n",
    "    transform = transforms.ToTensor()\n",
    "    train_data = torchvision.datasets.ImageFolder(img_dir, transform=transform)\n",
    "    print(train_data)\n",
    "    train_iter = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle,\n",
    "                                             num_workers=num_workers)\n",
    "    return train_iter, train_data.classes\n",
    "\n",
    "\n",
    "def generateImageVecFiles(imageSize=480, inChannel=3, batchSize=4, blockNum=196, kernelSize=64, stride=32,\n",
    "                          outputSize=2048):\n",
    "    \"\"\"\n",
    "    :param imageSize: 图像统一调整为多少\n",
    "    :param inChannel: 输入通道\n",
    "    :param batchSize:\n",
    "    :param blockNum: 一个图片分为多少个区域\n",
    "    :param kernelSize: 每个区域多大\n",
    "    :param stride: 步长\n",
    "    :param outputSize: 输出的向量多少\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    net = getResNet50().to(device=try_gpu())\n",
    "\n",
    "    if not os.path.exists(save_array_dir):\n",
    "        os.mkdir(save_array_dir)\n",
    "    else:\n",
    "        if len(os.listdir(save_array_dir)) >= 1:  # 说明已经有文件了 - 默认已经得到了向量\n",
    "            return None\n",
    "    if not os.path.exists(save_images_dir):\n",
    "        os.mkdir(save_images_dir)\n",
    "\n",
    "    ProcessPreImages(read_images_dir, imageSize, save_images_dir)\n",
    "    preVectorIter, classes = getDatasetIter(save_images_dir, batch_size=batchSize, shuffle=False)\n",
    "    extractImageFeature = ImageFeature(net=net, block_num=blockNum,\n",
    "                                       kernel_size=kernelSize, stride=stride,\n",
    "                                       output_size=outputSize, in_channel=inChannel)\n",
    "\n",
    "    def saveArray(array, index):\n",
    "        array = array.unsqueeze(0).detach().numpy()\n",
    "        numpy.save(save_array_dir + classes[int(index)], array)\n",
    "\n",
    "    count = 0\n",
    "    net.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for X, y in preVectorIter:\n",
    "            end = time.time()\n",
    "            if count % (batchSize * 50) == 0:\n",
    "                print(\"have got {} image vectors, total cost:{:.2f} min\".format(count, (end - start) / 60))\n",
    "            count += batchSize\n",
    "            if os.path.exists(save_array_dir + classes[int(y[0])] + \".npy\"):\n",
    "                continue\n",
    "            batch_tensor = extractImageFeature.forward(X.type(torch.float32).cuda()).to(torch.device('cpu'))\n",
    "            torch.cuda.empty_cache()\n",
    "            [saveArray(data, index) for data, index in zip(batch_tensor, y)]  # 加速\n",
    "\n",
    "\n",
    "def modelScoresVision(writer, scoresValues, scoresNames, lrValues=None):\n",
    "    \"\"\"\n",
    "    :param lrValues:\n",
    "    :param scoresNames:\n",
    "    :param writer: Tensoboard\n",
    "    :param scoresValues:  epochs * 评估参数个数 * 数据集字典\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if lrValues is not None:\n",
    "        for batch in range(len(lrValues)):\n",
    "            writer.add_scalar(tag=\"train_lr\", scalar_value=lrValues[batch], global_step=batch)\n",
    "\n",
    "    if scoresValues is None:\n",
    "        return None\n",
    "    for epoch in range(len(scoresValues) - 1):\n",
    "        for i in range(len(scoresNames)):\n",
    "            mapDict = {\n",
    "                \"train\": scoresValues[epoch][i][0],\n",
    "                \"test\": scoresValues[epoch][i][1],\n",
    "                \"valid\": scoresValues[epoch][i][2],\n",
    "            }\n",
    "            writer.add_scalars(main_tag=scoresNames[i], tag_scalar_dict=mapDict, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b24524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.089559Z",
     "iopub.status.busy": "2023-05-25T06:47:00.088956Z",
     "iopub.status.idle": "2023-05-25T06:47:00.095004Z",
     "shell.execute_reply": "2023-05-25T06:47:00.093696Z"
    },
    "papermill": {
     "duration": 0.016231,
     "end_time": "2023-05-25T06:47:00.097471",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.081240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/4 19:24\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : DATASET.py\n",
    "# @Description :\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class DATASET(Enum):\n",
    "    TRAIN = \"train_text\"\n",
    "    TEST = \"test_text\"\n",
    "    VALID = \"valid_text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0bfdde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.112634Z",
     "iopub.status.busy": "2023-05-25T06:47:00.111674Z",
     "iopub.status.idle": "2023-05-25T06:47:00.130155Z",
     "shell.execute_reply": "2023-05-25T06:47:00.129232Z"
    },
    "papermill": {
     "duration": 0.028617,
     "end_time": "2023-05-25T06:47:00.132590",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.103973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/24 15:02\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : LoadData.py\n",
    "# @Description : 读取各种数据\n",
    "\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, seqLen, imageClassDir, imageVectorDir, textDir, wordVocabDir, dataType=DATASET.TRAIN):\n",
    "        \"\"\"\n",
    "        :param seqLen:\n",
    "        :param imageClassDir:图片对应类的字典序列化文件\n",
    "        :param imageVectorDir:ResNet生成的文本向量的文件目录\n",
    "        :param textDir:推特数据的文本数据文件\n",
    "        :param wordVocabDir:词表的字典序列化文件\n",
    "        \"\"\"\n",
    "        self.sqLen = seqLen\n",
    "        mapDataSet = {\n",
    "            DATASET.TRAIN: \"train_text\",\n",
    "            DATASET.TEST: \"test_text\",\n",
    "            DATASET.VALID: \"valid_text\"\n",
    "        }\n",
    "        self.seqLen = seqLen\n",
    "        self.imageVectorDir = imageVectorDir\n",
    "        self.id2text = []\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(modelWightsDir + \"bert-base-cased\")\n",
    "        with open(wordVocabDir + \"vocab.py3\", 'rb') as f:\n",
    "            self.word2id = pickle.load(f)  # 词表\n",
    "        with open(imageClassDir + \"class2id.py3\", 'rb') as f:\n",
    "            self.attribute2id = pickle.load(f)  # 类表\n",
    "        with open(imageClassDir + \"image2class.py3\", 'rb') as f:\n",
    "            self.dictExtractWords = pickle.load(f)  # 类表\n",
    "        with open(textDir + mapDataSet[dataType], 'r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.id2text.append(eval(line))\n",
    "        self.id2text = numpy.array(self.id2text)\n",
    "\n",
    "    def processText(self, sqLen, source):\n",
    "        \"\"\"\n",
    "        :param sqLen:文本长度\n",
    "        :param source:字符串\n",
    "        :return:对应词表的对应SqLen长度\n",
    "        \"\"\"\n",
    "        strs = source.split(\" \")\n",
    "        if len(strs) > sqLen:\n",
    "            strs = strs[:sqLen]\n",
    "        strs = numpy.array(strs)\n",
    "        func = numpy.vectorize(lambda x: self.word2id[x] if x in self.word2id else self.word2id['<unk>'])\n",
    "        return numpy.pad(func(strs), (0, sqLen - len(strs)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.id2text[index, 0]\n",
    "        text = ' '.join(self.dictExtractWords[int(id)]) + self.id2text[index, 1]\n",
    "        reText = torch.tensor(self.processText(self.seqLen, text))\n",
    "        retY = torch.tensor(self.id2text[index, 2].astype(numpy.float32))\n",
    "        reWords = torch.tensor(itemgetter(*self.dictExtractWords[int(id)])(self.attribute2id))\n",
    "        image = torch.tensor(numpy.load(self.imageVectorDir + id + \".npy\").squeeze())\n",
    "        encodedInput = self.tokenizer(text, return_tensors='pt', padding=\"max_length\", max_length=self.sqLen,\n",
    "                                      truncation=True)\n",
    "        input_ids, token_type_ids, attention_mask = encodedInput[\"input_ids\"], encodedInput[\n",
    "            \"token_type_ids\"], encodedInput[\"attention_mask\"]\n",
    "        return (reText, image, reWords, (input_ids, token_type_ids, attention_mask)), retY\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.id2text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c0a495c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.147547Z",
     "iopub.status.busy": "2023-05-25T06:47:00.146686Z",
     "iopub.status.idle": "2023-05-25T06:47:00.159286Z",
     "shell.execute_reply": "2023-05-25T06:47:00.158351Z"
    },
    "papermill": {
     "duration": 0.022442,
     "end_time": "2023-05-25T06:47:00.161602",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.139160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/25 16:13\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ExtractFeature.py\n",
    "# @Description :图像向量部\n",
    "\n",
    "\n",
    "class ExtractFeature(nn.Module):\n",
    "    def __init__(self, embeddingDir, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.embeddingArray = torch.Tensor(\n",
    "            numpy.loadtxt(embeddingDir, delimiter=\" \", dtype=\"float32\"))  # 1001 * 300 1001为1000个类和1个unk\n",
    "        if device == \"gpu\":\n",
    "            self.embeddingArray = self.embeddingArray.to(try_gpu())\n",
    "        self.embSize = self.embeddingArray.shape[1]  # 向量后的大小\n",
    "        self.vocabSize = self.embeddingArray.shape[0]  # 类表大小\n",
    "        self.embedding = nn.Embedding(self.vocabSize, self.embSize).from_pretrained(self.embeddingArray)\n",
    "        self.linear1 = nn.Linear(self.embSize, self.embSize // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.embSize // 2, 1)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.attention = AdditiveAttention(key_size=self.embSize, query_size=self.embSize,\n",
    "                                           num_hiddens=self.embSize // 2)\n",
    "        nn.init.kaiming_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, classes = X.shape[0], X.shape[1]\n",
    "        output1 = self.embedding(X)  # batch * 5 * 200\n",
    "        # # 这里也可以用之前写的注意力机制 两个版本\n",
    "        return output1, torch.mean(self.attention.forward(queries=output1, keys=output1, values=output1),\n",
    "                                   dim=1).squeeze()\n",
    "        # output2 = self.relu(self.linear1(output1))  # batch * 5 * 100\n",
    "        # output3 = self.softmax(self.linear2(output2)).reshape(batch_size, 1, classes)  # batch * 1 * 5\n",
    "        # return output1, torch.squeeze(output3 @ output1)  # batch * 5 * 100, batch * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3088ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.176662Z",
     "iopub.status.busy": "2023-05-25T06:47:00.175639Z",
     "iopub.status.idle": "2023-05-25T06:47:00.184352Z",
     "shell.execute_reply": "2023-05-25T06:47:00.183418Z"
    },
    "papermill": {
     "duration": 0.018472,
     "end_time": "2023-05-25T06:47:00.186629",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.168157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/25 19:20\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ImageFeature.py\n",
    "# @Description :对于图片提取出来的向量加入网络\n",
    "\n",
    "class ImageFeature(nn.Module):\n",
    "    def __init__(self, defaultFeatureSize=1024, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.defaultFeatureSize = defaultFeatureSize\n",
    "        self.linear = torch.nn.Linear(2048, self.defaultFeatureSize)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # batch_size = X.shape[0]\n",
    "        output = self.relu(self.linear(X))\n",
    "        return output, torch.mean(output, dim=1)  # batch * 196 *1024,  batch  * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861a6c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.201420Z",
     "iopub.status.busy": "2023-05-25T06:47:00.201043Z",
     "iopub.status.idle": "2023-05-25T06:47:00.233535Z",
     "shell.execute_reply": "2023-05-25T06:47:00.232419Z"
    },
    "papermill": {
     "duration": 0.042761,
     "end_time": "2023-05-25T06:47:00.235886",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.193125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/25 19:33\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : TextFeature.py\n",
    "# @Description :文本特征提取\n",
    "\n",
    "\n",
    "class TextFeature_LSTM(nn.Module):\n",
    "    def __init__(self, nHidden, seqLen, guideLen, textEmbeddingDir, numLayers=1, dropout=0, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        :param nHidden: 隐藏层\n",
    "        :param seqLen:  步长\n",
    "        :param guideLen: 引导向量的维度 - 物品类别嵌入后的维度\n",
    "        :param textEmbeddingDir: 文本glove后的向量\n",
    "        :param numLayers: 网络层数 - 构建深层网络结构\n",
    "        :param dropout:\n",
    "        \"\"\"\n",
    "        super(TextFeature_LSTM, self).__init__()\n",
    "        self.nHidden = nHidden\n",
    "        self.seqLen = seqLen\n",
    "        self.numLayers = numLayers\n",
    "        self.dropout = dropout\n",
    "        self.guideLen = guideLen\n",
    "        self.embeddingArray = torch.Tensor(\n",
    "            numpy.loadtxt(textEmbeddingDir, delimiter=\" \", dtype=\"float32\"))\n",
    "        if device == \"gpu\":\n",
    "            self.embeddingArray = self.embeddingArray.to(try_gpu())\n",
    "        self.embSize = self.embeddingArray.shape[1]  # 向量后的大小\n",
    "        self.vocabSize = self.embeddingArray.shape[0]  # 类表大小\n",
    "        self.embedding = nn.Embedding(self.vocabSize, self.embSize).from_pretrained(self.embeddingArray)\n",
    "        self.layerNorm = nn.LayerNorm(self.embSize)\n",
    "        self.fwLinearH = torch.nn.Linear(guideLen, self.nHidden)\n",
    "        self.fwLinearC = torch.nn.Linear(guideLen, self.nHidden)\n",
    "        self.bwLinearH = torch.nn.Linear(guideLen, self.nHidden)\n",
    "        self.bwLinearC = torch.nn.Linear(guideLen, self.nHidden)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.biLSTM = nn.LSTM(input_size=self.embSize,\n",
    "                              hidden_size=self.nHidden,\n",
    "                              batch_first=True,\n",
    "                              num_layers=self.numLayers,\n",
    "                              dropout=self.dropout,\n",
    "                              # 没有加dropout 因为不知道如何在测试时取消 20230426 - 预测时model.eval() / model.train() 来控制 20230427\n",
    "                              bidirectional=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        # 默认方法\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "        if isinstance(m, nn.LSTM):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.xavier_normal_(param)\n",
    "\n",
    "    def forward(self, X, guideVector):\n",
    "        \"\"\"\n",
    "        :param X:\n",
    "        :param guideVector:  引导向量的 - 物品类别嵌入后\n",
    "        :return: (batch_size, 2 * nHidden)\n",
    "        \"\"\"\n",
    "        if guideVector is None:\n",
    "            guideVector = torch.zeros((len(X), self.guideLen)).cuda()\n",
    "        X = self.embedding(X)\n",
    "        X = self.layerNorm(X)\n",
    "        fw_h0 = self.relu(self.fwLinearH(guideVector))\n",
    "        fw_c0 = self.relu(self.fwLinearC(guideVector))\n",
    "        bw_h0 = self.relu(self.bwLinearH(guideVector))\n",
    "        # bw_h0 = self.relu(self.fwLinearH(guideVector)) # 这里是否使用同一感知机层初始化正向和反向的H，C，需要进一步实验 20230427\n",
    "        bw_c0 = self.relu(self.bwLinearC(guideVector))\n",
    "        # bw_c0 = self.relu(self.fwLinearC(guideVector))\n",
    "        init_h0 = torch.stack((fw_h0,) * self.numLayers + (bw_h0,) * self.numLayers,\n",
    "                              dim=0)  # 深层LSTM是初始化为(D * layer , nHidden) -> (D, layers, nHidden) 观察API得出 存疑20230427\n",
    "        init_c0 = torch.stack((fw_c0,) * self.numLayers + (bw_c0,) * self.numLayers,\n",
    "                              dim=0)  # 加入stack 后网络的感知层是否会更新？ 20230427\n",
    "        output, (_, _) = self.biLSTM(X, (init_h0, init_c0))  # output = batch_size * seqLen * (2 * hidden)\n",
    "        return output, torch.mean(output, dim=1)  # batch_size * seqLen * (2 * hidden), batch_size * (2 * hidden)\n",
    "\n",
    "\n",
    "class TextFeature_Bert(nn.Module):\n",
    "    def __init__(self, nHidden, sqLen, dropout, device=\"cpu\"):\n",
    "        super(TextFeature_Bert, self).__init__()\n",
    "        self.device = device\n",
    "        self.nHidden = nHidden\n",
    "        self.sqLen = sqLen\n",
    "        self.bert = BertModel.from_pretrained(modelWightsDir + \"bert-base-cased\")\n",
    "        self.layerNorm = nn.LayerNorm(768)  # 模型一般是768 如果是别的自己改一下\n",
    "        self.linear = nn.Linear(768, self.nHidden * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        # 默认方法\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, text):\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "        with torch.no_grad():\n",
    "            output = self.bert(input_ids=input_ids.squeeze(), token_type_ids=token_type_ids.squeeze(),\n",
    "                               attention_mask=attention_mask.squeeze())[0].detach()\n",
    "        output = self.layerNorm(output)\n",
    "        output = self.tanh(self.linear(output))\n",
    "        output = self.dropout(output)\n",
    "        return output, torch.mean(output, dim=1)\n",
    "\n",
    "\n",
    "class TextFeature(nn.Module):\n",
    "    def __init__(self, nHidden, seqLen, guideLen, textEmbeddingDir, numLayers=1, dropout=0, device=\"cpu\"):\n",
    "        super(TextFeature, self).__init__()\n",
    "        self.nHidden = nHidden\n",
    "        self.lstm = TextFeature_LSTM(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                     numLayers=numLayers,\n",
    "                                     guideLen=guideLen, dropout=dropout, device=device)\n",
    "        self.bert = TextFeature_Bert(nHidden=nHidden, sqLen=seqLen, dropout=dropout)\n",
    "        self.attentionLSTM = AdditiveAttention(query_size=nHidden * 2, key_size=nHidden * 2, dropout=dropout,\n",
    "                                               num_hiddens=nHidden)\n",
    "        self.elu = nn.ELU()\n",
    "        self.lstm.apply(self.lstm.weight_init)\n",
    "        self.bert.apply(self.bert.weight_init)\n",
    "        self.attentionLSTM.apply(self.attentionLSTM.weight_init)\n",
    "\n",
    "    def forward(self, reText, text, guideVector):\n",
    "        lstm_o, lstm_vec = self.lstm.forward(reText, guideVector)\n",
    "        bert_o, bert_vec = self.bert(text)\n",
    "        lstm_o = self.attentionLSTM.forward(lstm_o, lstm_o, lstm_o)\n",
    "        o = (lstm_o + bert_o) / 2\n",
    "        output = self.elu(o)\n",
    "        output_vec = (lstm_vec + bert_vec) / 2\n",
    "        return output, output_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7454a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.250942Z",
     "iopub.status.busy": "2023-05-25T06:47:00.250621Z",
     "iopub.status.idle": "2023-05-25T06:47:00.277061Z",
     "shell.execute_reply": "2023-05-25T06:47:00.276074Z"
    },
    "papermill": {
     "duration": 0.037045,
     "end_time": "2023-05-25T06:47:00.279592",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.242547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/4 9:58\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : Attention.py\n",
    "# @Description :各种注意力机制\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"\n",
    "     通过在最后一个轴上掩蔽元素来执行softmax操作\n",
    "    :param X: X:3D张量\n",
    "    :param valid_lens: valid_lens:1D或2D张量\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                          value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout=0):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        # 默认方法\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # 在维度扩展后，\n",
    "        # queries的形状：(batchSize，查询的个数，1，num_hidden)\n",
    "        # key的形状：(batchSize，1，“键－值”对的个数，num_hiddens)\n",
    "        # 使用广播方式进行求和\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。\n",
    "        # scores的形状：(batchSize，查询的个数，“键-值”对的个数)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # values的形状：(batchSize，“键－值”对的个数，值的维度)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.attention_weights = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        \"\"\"\n",
    "        :param valid_lens: 忽略某些键值对时用\n",
    "        :param X: (batchSize，查询的个数，d)\n",
    "        :return: batchSize * 值的维度\n",
    "        \"\"\"\n",
    "        #  queries: (batchSize，查询的个数，d)\n",
    "        #  keys: (batchSize，“键－值”对的个数，d)\n",
    "        #  values: (batchSize，“键－值”对的个数，值的维度)\n",
    "        #  valid_lens: (batchSize，查询的个数)\n",
    "        queries, keys, values = X, X, X\n",
    "        d = queries.shape[-1]\n",
    "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)  # batchSize，查询个数的个数，d\n",
    "\n",
    "\n",
    "class MultiModalAttention(nn.Module):\n",
    "    \"\"\"多模态加性注意力机制融合\"\"\"\n",
    "\n",
    "    def __init__(self, querySizes, keySize, dropout=0):\n",
    "        \"\"\"\n",
    "        QKV: query = query, key=key, value=key\n",
    "        :param querySizes: 利用多个向量进行融合-源于\n",
    "        :param keySize:\n",
    "        :param dropout:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attentions = []\n",
    "        count = 0\n",
    "        for querySize in querySizes:\n",
    "            exec(\n",
    "                \"self.addATT_{} = AdditiveAttention(query_size=querySize, key_size=keySize, num_hiddens=keySize // 2, \"\n",
    "                \"dropout=dropout)\".format(\n",
    "                    count))\n",
    "            exec(\"self.attentions.append(self.addATT_{})\".format(count))\n",
    "            count += 1\n",
    "        [attention.apply(MultiModalAttention.weight_init) for attention in self.attentions]\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, queries, key):\n",
    "        \"\"\"\n",
    "        :param queries: (query1, query2...)\n",
    "        :param key: batchSize, 键值对, values\n",
    "        :return: batchSize * 1 * 值的维度\n",
    "        \"\"\"\n",
    "\n",
    "        vector = torch.zeros(key.shape[0], 1, key.shape[-1], device=key.device)  # 这里加维为了后面stack\n",
    "        for attention, query in zip(self.attentions, queries):\n",
    "            vector += attention.forward(queries=query, keys=key, values=key)\n",
    "        return vector / len(self.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19d02b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.295126Z",
     "iopub.status.busy": "2023-05-25T06:47:00.294784Z",
     "iopub.status.idle": "2023-05-25T06:47:00.320967Z",
     "shell.execute_reply": "2023-05-25T06:47:00.320014Z"
    },
    "papermill": {
     "duration": 0.036992,
     "end_time": "2023-05-25T06:47:00.323403",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.286411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeleteEFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nHidden, seqLen, dropout=0, numLayers=1, classEmbeddingDir=\"..//ExtractWords/vector\",\n",
    "                 textEmbeddingDir=\"../words/vector\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.FinalMLPSize = 512\n",
    "        self.device = device\n",
    "        self.extractFeature = ExtractFeature(embeddingDir=classEmbeddingDir, device=device)  # 图像中物品类别\n",
    "        self.imageFeature = ImageFeature()  # 图像特征\n",
    "        self.imageFeature.apply(ImageFeature.weight_init)\n",
    "        self.textFeature = TextFeature(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                       numLayers=numLayers,\n",
    "                                       guideLen=self.extractFeature.embSize, dropout=dropout, device=device)\n",
    "\n",
    "        # 注意力机制以 x, y, z 指导向量计算与 key的评分，最后将其平均 这里用的是加性注意力机制，由seqToSeq翻译的注意力所启发\n",
    "        self.extractFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.extractFeature.embSize, dropout=dropout)\n",
    "        self.imageFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.imageFeature.defaultFeatureSize, dropout=dropout)\n",
    "        self.textFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.textFeature.nHidden * 2, dropout=dropout)\n",
    "\n",
    "        # 为了后面的缩放点积注意力，需要把多模态向量调整为同一维度，后加入注意力机制，减少模型复杂度\n",
    "        self.extractLinear = nn.Linear(self.extractFeature.embSize, self.FinalMLPSize)\n",
    "        self.extractRelu = nn.ReLU()\n",
    "        self.imageLinear = nn.Linear(self.imageFeature.defaultFeatureSize, self.FinalMLPSize)\n",
    "        self.imageRelu = nn.ReLU()\n",
    "        self.textLinear = nn.Linear(self.textFeature.nHidden * 2, self.FinalMLPSize)\n",
    "        self.textRelu = nn.ReLU()\n",
    "\n",
    "        self.multiAttention = DotProductAttention(dropout=dropout)\n",
    "\n",
    "        # 最后加入两层全连接层\n",
    "        self.MLP, self.FC = nn.Linear(self.FinalMLPSize, self.FinalMLPSize // 2), nn.Linear(self.FinalMLPSize // 2, 1)\n",
    "        self.mlpRelu, self.fcSigmoid = nn.ReLU(), nn.Sigmoid()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reText, images, reWords, text = X\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "\n",
    "        if self.device == \"gpu\":\n",
    "            reText, images, reWords = reText.to(try_gpu()), images.to(try_gpu()), reWords.to(\n",
    "                try_gpu())\n",
    "            input_ids, token_type_ids, attention_mask = input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda()\n",
    "\n",
    "        extractMatrix, extractGuidVec = self.extractFeature.forward(reWords)\n",
    "        imageMatrix, imageGuidVec = self.imageFeature.forward(images)\n",
    "        # 改动 -------------------------------------------------------------------->\n",
    "        extractGuidVec = torch.zeros_like(extractGuidVec)  # 这里使输入为零使早期融合无效\n",
    "        # <---------------------------------------------------------------------\n",
    "        textHMatrix, textGuidVec = self.textFeature.forward(reText, (input_ids, token_type_ids, attention_mask),\n",
    "                                                            extractGuidVec)\n",
    "        extractGuidVec, imageGuidVec, textGuidVec = extractGuidVec.unsqueeze(1), imageGuidVec.unsqueeze(\n",
    "            1), textGuidVec.unsqueeze(1)  # 升维\n",
    "        extractVec = self.extractFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), extractMatrix)\n",
    "        imageVec = self.imageFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), imageMatrix)\n",
    "        textVec = self.textFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), textHMatrix)  # QVA\n",
    "\n",
    "        extractVec, imageVec, textVec = extractVec.squeeze(1), imageVec.squeeze(1), textVec.squeeze(1)  # 降维\n",
    "\n",
    "        # 是否加入relu继续激活 未实验 20230504\n",
    "        extractVec = self.extractLinear.forward(extractVec)\n",
    "        extractVec = self.extractRelu(extractVec)\n",
    "        imageVec = self.imageLinear.forward(imageVec)\n",
    "        imageVec = self.imageRelu(imageVec)\n",
    "        textVec = self.textLinear.forward(textVec)\n",
    "        textVec = self.textRelu(textVec)\n",
    "        finalMatrix = torch.stack((extractVec, imageVec, textVec), dim=1)  # 转化为 batch * 3 * FinalMLPSize\n",
    "        finalVec = torch.mean(self.multiAttention.forward(finalMatrix), dim=1)\n",
    "        fcInput = self.mlpRelu(self.MLP(finalVec))\n",
    "        return self.fcSigmoid(self.FC(fcInput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "410eeb22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.338603Z",
     "iopub.status.busy": "2023-05-25T06:47:00.338286Z",
     "iopub.status.idle": "2023-05-25T06:47:00.362892Z",
     "shell.execute_reply": "2023-05-25T06:47:00.361842Z"
    },
    "papermill": {
     "duration": 0.034911,
     "end_time": "2023-05-25T06:47:00.365091",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.330180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/4/27 11:11\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : NNManager.py\n",
    "# @Description :总体网络搭建\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, nHidden, seqLen, dropout=0, numLayers=1, classEmbeddingDir=\"..//ExtractWords/vector\",\n",
    "                 textEmbeddingDir=\"../words/vector\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.FinalMLPSize = 512\n",
    "        self.device = device\n",
    "        self.extractFeature = ExtractFeature(embeddingDir=classEmbeddingDir, device=device)  # 图像中物品类别\n",
    "        self.imageFeature = ImageFeature()  # 图像特征\n",
    "        self.imageFeature.apply(ImageFeature.weight_init)\n",
    "        self.textFeature = TextFeature(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                       numLayers=numLayers,\n",
    "                                       guideLen=self.extractFeature.embSize, dropout=dropout, device=device)\n",
    "\n",
    "        # 注意力机制以 x, y, z 指导向量计算与 key的评分，最后将其平均 这里用的是加性注意力机制\n",
    "        self.extractFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.extractFeature.embSize, dropout=dropout)\n",
    "        self.imageFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.imageFeature.defaultFeatureSize, dropout=dropout)\n",
    "        self.textFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.textFeature.nHidden * 2, dropout=dropout)\n",
    "\n",
    "        # 为了后面的缩放点积注意力，需要把多模态向量调整为同一维度，后加入注意力机制\n",
    "        self.extractLinear = nn.Linear(self.extractFeature.embSize, self.FinalMLPSize)\n",
    "        self.extractRelu = nn.ReLU()\n",
    "        self.imageLinear = nn.Linear(self.imageFeature.defaultFeatureSize, self.FinalMLPSize)\n",
    "        self.imageRelu = nn.ReLU()\n",
    "        self.textLinear = nn.Linear(self.textFeature.nHidden * 2, self.FinalMLPSize)\n",
    "        self.textRelu = nn.ReLU()\n",
    "\n",
    "        self.multiAttention = DotProductAttention(dropout=dropout)\n",
    "\n",
    "        # 最后加入两层全连接层\n",
    "        self.MLP, self.FC = nn.Linear(self.FinalMLPSize, self.FinalMLPSize // 2), nn.Linear(self.FinalMLPSize // 2, 1)\n",
    "        self.mlpRelu, self.fcSigmoid = nn.ReLU(), nn.Sigmoid()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reText, images, reWords, text = X\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "\n",
    "        if self.device == \"gpu\":\n",
    "            reText, images, reWords = reText.to(try_gpu()), images.to(try_gpu()), reWords.to(\n",
    "                try_gpu())\n",
    "            input_ids, token_type_ids, attention_mask = input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda()\n",
    "\n",
    "        extractMatrix, extractGuidVec = self.extractFeature.forward(reWords)\n",
    "        imageMatrix, imageGuidVec = self.imageFeature.forward(images)\n",
    "        textHMatrix, textGuidVec = self.textFeature.forward(reText, (input_ids, token_type_ids, attention_mask),\n",
    "                                                            extractGuidVec)\n",
    "        extractGuidVec, imageGuidVec, textGuidVec = extractGuidVec.unsqueeze(1), imageGuidVec.unsqueeze(\n",
    "            1), textGuidVec.unsqueeze(1)  # 升维\n",
    "        extractVec = self.extractFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), extractMatrix)\n",
    "        imageVec = self.imageFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), imageMatrix)\n",
    "        textVec = self.textFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), textHMatrix)\n",
    "\n",
    "        extractVec, imageVec, textVec = extractVec.squeeze(1), imageVec.squeeze(1), textVec.squeeze(1)  # 降维\n",
    "\n",
    "        # 是否加入relu继续激活 未实验 20230504\n",
    "        extractVec = self.extractLinear.forward(extractVec)\n",
    "        extractVec = self.extractRelu(extractVec)\n",
    "        imageVec = self.imageLinear.forward(imageVec)\n",
    "        imageVec = self.imageRelu(imageVec)\n",
    "        textVec = self.textLinear.forward(textVec)\n",
    "        textVec = self.textRelu(textVec)\n",
    "        finalMatrix = torch.stack((extractVec, imageVec, textVec), dim=1)  # 转化为 batch * 3 * FinalMLPSize\n",
    "        # print(\"finalMatrix.shape\", finalMatrix.shape)\n",
    "        finalVec = torch.mean(self.multiAttention.forward(finalMatrix), dim=1)\n",
    "        # print(\"finalVec.shape\", finalVec.shape)\n",
    "        fcInput = self.mlpRelu(self.MLP(finalVec))\n",
    "        # print(self.FC.weight.grad)\n",
    "        return self.fcSigmoid(self.FC(fcInput))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d020e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.380168Z",
     "iopub.status.busy": "2023-05-25T06:47:00.379832Z",
     "iopub.status.idle": "2023-05-25T06:47:00.403373Z",
     "shell.execute_reply": "2023-05-25T06:47:00.402449Z"
    },
    "papermill": {
     "duration": 0.033849,
     "end_time": "2023-05-25T06:47:00.405633",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.371784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/17 15:29\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : DeleteRFNet.py\n",
    "# @Description :删除了表示融合的网络\n",
    "\n",
    "class DeleteRFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nHidden, seqLen, dropout=0, numLayers=1, classEmbeddingDir=\"..//ExtractWords/vector\",\n",
    "                 textEmbeddingDir=\"../words/vector\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.FinalMLPSize = 512\n",
    "        self.device = device\n",
    "        self.extractFeature = ExtractFeature(embeddingDir=classEmbeddingDir, device=device)  # 图像中物品类别\n",
    "        self.imageFeature = ImageFeature()  # 图像特征\n",
    "        self.imageFeature.apply(ImageFeature.weight_init)\n",
    "        self.textFeature = TextFeature(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                       numLayers=numLayers,\n",
    "                                       guideLen=self.extractFeature.embSize, dropout=dropout, device=device)\n",
    "\n",
    "        # 注意力机制以 x, y, z 指导向量计算与 key的评分，最后将其平均 这里用的是加性注意力机制，由seqToSeq翻译的注意力所启发\n",
    "        self.extractFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.extractFeature.embSize, dropout=dropout)\n",
    "        self.imageFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.imageFeature.defaultFeatureSize, dropout=dropout)\n",
    "        self.textFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.textFeature.nHidden * 2, dropout=dropout)\n",
    "\n",
    "        # 为了后面的缩放点积注意力，需要把多模态向量调整为同一维度，后加入注意力机制，减少模型复杂度\n",
    "        self.extractLinear = nn.Linear(self.extractFeature.embSize, self.FinalMLPSize)\n",
    "        self.extractRelu = nn.ReLU()\n",
    "        self.imageLinear = nn.Linear(self.imageFeature.defaultFeatureSize, self.FinalMLPSize)\n",
    "        self.imageRelu = nn.ReLU()\n",
    "        self.textLinear = nn.Linear(self.textFeature.nHidden * 2, self.FinalMLPSize)\n",
    "        self.textRelu = nn.ReLU()\n",
    "\n",
    "        self.multiAttention = DotProductAttention(dropout=dropout)\n",
    "\n",
    "        # 最后加入两层全连接层\n",
    "        self.MLP, self.FC = nn.Linear(self.FinalMLPSize, self.FinalMLPSize // 2), nn.Linear(self.FinalMLPSize // 2, 1)\n",
    "        self.mlpRelu, self.fcSigmoid = nn.ReLU(), nn.Sigmoid()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reText, images, reWords, text = X\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "\n",
    "        if self.device == \"gpu\":\n",
    "            reText, images, reWords = reText.to(try_gpu()), images.to(try_gpu()), reWords.to(\n",
    "                try_gpu())\n",
    "            input_ids, token_type_ids, attention_mask = input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda()\n",
    "\n",
    "        extractMatrix, extractGuidVec = self.extractFeature.forward(reWords)\n",
    "        imageMatrix, imageGuidVec = self.imageFeature.forward(images)\n",
    "        textHMatrix, textGuidVec = self.textFeature.forward(reText, (input_ids, token_type_ids, attention_mask),\n",
    "                                                            extractGuidVec)\n",
    "        # 改动 --------------------------------------------------------------------------------------->\n",
    "        extractVec, imageVec, textVec = extractGuidVec, imageGuidVec, textGuidVec  # 降维\n",
    "        # <----------------------------------------------------------------------------------------\n",
    "\n",
    "        # 是否加入relu继续激活 未实验 20230504\n",
    "        extractVec = self.extractLinear.forward(extractVec)\n",
    "        extractVec = self.extractRelu(extractVec)\n",
    "        imageVec = self.imageLinear.forward(imageVec)\n",
    "        imageVec = self.imageRelu(imageVec)\n",
    "        textVec = self.textLinear.forward(textVec)\n",
    "        textVec = self.textRelu(textVec)\n",
    "        finalMatrix = torch.stack((extractVec, imageVec, textVec), dim=1)  # 转化为 batch * 3 * FinalMLPSize\n",
    "        finalVec = torch.mean(self.multiAttention.forward(finalMatrix), dim=1)\n",
    "        fcInput = self.mlpRelu(self.MLP(finalVec))\n",
    "        return self.fcSigmoid(self.FC(fcInput))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "245dca25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.420580Z",
     "iopub.status.busy": "2023-05-25T06:47:00.420274Z",
     "iopub.status.idle": "2023-05-25T06:47:00.445308Z",
     "shell.execute_reply": "2023-05-25T06:47:00.444063Z"
    },
    "papermill": {
     "duration": 0.035583,
     "end_time": "2023-05-25T06:47:00.447890",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.412307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/5/17 15:32\n",
    "# @Author  : CaoQixuan\n",
    "# @File    : ReplaceEFNet.py\n",
    "# @Description :利用图像引导向量替代早期融合时的类别引导向量\n",
    "\n",
    "\n",
    "class ReplaceEFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nHidden, seqLen, dropout=0, numLayers=1, classEmbeddingDir=\"..//ExtractWords/vector\",\n",
    "                 textEmbeddingDir=\"../words/vector\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.FinalMLPSize = 512\n",
    "        self.device = device\n",
    "        self.extractFeature = ExtractFeature(embeddingDir=classEmbeddingDir, device=device)  # 图像中物品类别\n",
    "        self.imageFeature = ImageFeature()  # 图像特征\n",
    "        self.imageFeature.apply(ImageFeature.weight_init)\n",
    "        self.textFeature = TextFeature(nHidden, seqLen, textEmbeddingDir=textEmbeddingDir,\n",
    "                                       numLayers=numLayers,\n",
    "                                       guideLen=self.imageFeature.defaultFeatureSize, dropout=dropout, device=device)\n",
    "\n",
    "        # 注意力机制以 x, y, z 指导向量计算与 key的评分，最后将其平均 这里用的是加性注意力机制，由seqToSeq翻译的注意力所启发\n",
    "        self.extractFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.extractFeature.embSize, dropout=dropout)\n",
    "        self.imageFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.imageFeature.defaultFeatureSize, dropout=dropout)\n",
    "        self.textFeatureATT = MultiModalAttention(\n",
    "            querySizes=(\n",
    "                self.extractFeature.embSize, self.imageFeature.defaultFeatureSize, self.textFeature.nHidden * 2),\n",
    "            keySize=self.textFeature.nHidden * 2, dropout=dropout)\n",
    "\n",
    "        # 为了后面的缩放点积注意力，需要把多模态向量调整为同一维度，后加入注意力机制，减少模型复杂度\n",
    "        self.extractLinear = nn.Linear(self.extractFeature.embSize, self.FinalMLPSize)\n",
    "        self.extractRelu = nn.ReLU()\n",
    "        self.imageLinear = nn.Linear(self.imageFeature.defaultFeatureSize, self.FinalMLPSize)\n",
    "        self.imageRelu = nn.ReLU()\n",
    "        self.textLinear = nn.Linear(self.textFeature.nHidden * 2, self.FinalMLPSize)\n",
    "        self.textRelu = nn.ReLU()\n",
    "\n",
    "        self.multiAttention = DotProductAttention(dropout=dropout)\n",
    "\n",
    "        # 最后加入两层全连接层\n",
    "        self.MLP, self.FC = nn.Linear(self.FinalMLPSize, self.FinalMLPSize // 2), nn.Linear(self.FinalMLPSize // 2, 1)\n",
    "        self.mlpRelu, self.fcSigmoid = nn.ReLU(), nn.Sigmoid()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reText, images, reWords, text = X\n",
    "        input_ids, token_type_ids, attention_mask = text\n",
    "\n",
    "        if self.device == \"gpu\":\n",
    "            reText, images, reWords = reText.to(try_gpu()), images.to(try_gpu()), reWords.to(\n",
    "                try_gpu())\n",
    "            input_ids, token_type_ids, attention_mask = input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda()\n",
    "\n",
    "        extractMatrix, extractGuidVec = self.extractFeature.forward(reWords)\n",
    "        imageMatrix, imageGuidVec = self.imageFeature.forward(images)\n",
    "        # 改动 -------------------------------------------------------------------->\n",
    "        textHMatrix, textGuidVec = self.textFeature.forward(reText, (input_ids, token_type_ids, attention_mask),\n",
    "                                                            imageGuidVec)\n",
    "        # <---------------------------------------------------------------------\n",
    "        extractGuidVec, imageGuidVec, textGuidVec = extractGuidVec.unsqueeze(1), imageGuidVec.unsqueeze(\n",
    "            1), textGuidVec.unsqueeze(1)  # 升维\n",
    "        extractVec = self.extractFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), extractMatrix)\n",
    "        imageVec = self.imageFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), imageMatrix)\n",
    "        textVec = self.textFeatureATT.forward((extractGuidVec, imageGuidVec, textGuidVec), textHMatrix)  # QVA\n",
    "\n",
    "        extractVec, imageVec, textVec = extractVec.squeeze(1), imageVec.squeeze(1), textVec.squeeze(1)  # 降维\n",
    "\n",
    "        # 是否加入relu继续激活 未实验 20230504\n",
    "        extractVec = self.extractLinear.forward(extractVec)\n",
    "        extractVec = self.extractRelu(extractVec)\n",
    "        imageVec = self.imageLinear.forward(imageVec)\n",
    "        imageVec = self.imageRelu(imageVec)\n",
    "        textVec = self.textLinear.forward(textVec)\n",
    "        textVec = self.textRelu(textVec)\n",
    "        finalMatrix = torch.stack((extractVec, imageVec, textVec), dim=1)  # 转化为 batch * 3 * FinalMLPSize\n",
    "        finalVec = torch.mean(self.multiAttention.forward(finalMatrix), dim=1)\n",
    "        fcInput = self.mlpRelu(self.MLP(finalVec))\n",
    "        return self.fcSigmoid(self.FC(fcInput))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ece57be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.463417Z",
     "iopub.status.busy": "2023-05-25T06:47:00.463072Z",
     "iopub.status.idle": "2023-05-25T06:47:00.506303Z",
     "shell.execute_reply": "2023-05-25T06:47:00.505182Z"
    },
    "papermill": {
     "duration": 0.05417,
     "end_time": "2023-05-25T06:47:00.508865",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.454695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over\n"
     ]
    }
   ],
   "source": [
    "class Main:\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.lr = 1e-5 * 2  # 学习率\n",
    "        self.nHidden = 256  # 隐藏层 - Bi-LSTM\n",
    "        self.seqLen = 80  # 步长 - Bi-LSTM\n",
    "        self.numLayers = 2  # 隐藏层层数\n",
    "        self.batchSize = 128  # 批量\n",
    "        self.maxClipping = 10  # 梯度裁剪\n",
    "        self.normType = 2  # 梯度的范式\n",
    "        self.dropout = 0.1  # DropOut层的概率 留取80%\n",
    "        self.maxEpoch = 100  # 最大迭代\n",
    "        self.displayStep = 1  # 多少轮后展示训练结果ExtractFeature.py  =1时 会记录每个人epoch 当!=1时 记录maxEpoch//displayStep\n",
    "        self.maxPatience = 40  # 能够容忍多少个epoch内都没有improvement 后期也不用了前期可调\n",
    "        self.representationScores = {}\n",
    "        self.lrRecord = []  # 记录学习率变化\n",
    "        self.scoreNames = [\"acc\", \"pre\", \"rec\", \"f1\", \"auc\", \"loss\"]\n",
    "        self.XExample = None  # 获得某一个X的样本\n",
    "        self.device = device\n",
    "        self.beforeEpoch = 0  # 可以继续训练\n",
    "        self.net = ReplaceEFNet(self.nHidden, self.seqLen, dropout=self.dropout, classEmbeddingDir=classEmbeddingDir,\n",
    "                       textEmbeddingDir=textEmbeddingDir, device=device, numLayers=self.numLayers)\n",
    "        self.net.apply(Net.weight_init)\n",
    "        self.loss = nn.BCELoss(reduction='none')\n",
    "        self.updater = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "        # 下面两个学习率衰减用法不一样\n",
    "        # self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #     optimizer=self.updater,\n",
    "        #     mode=\"min\",  # 增加/ 减小\n",
    "        #     patience=20,  # loos/acc 不再减小（或增大）的累计次数后改变学习率；\n",
    "        #     verbose=False,  # 是否可视\n",
    "        #     min_lr=1e-7,  # 最小的学习率\n",
    "        #     cooldown=10,  # 更新后冷静期\n",
    "        #     eps=1e-3  # If the difference between new and old lr is smaller than eps, the update is ignored\n",
    "        # )  # 在发现loss不再降低或者acc不再提高之后，降低学习率，这里用于批量的，所以呢，循环论数很多， 大约是 20K / batch_size\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer=self.updater,\n",
    "            T_0=5,  # 初试周期\n",
    "            T_mult=2\n",
    "        )  # 按cos函数下降 会周期性回复上来\n",
    "\n",
    "        self.train_iter = self.loadData(DATASET.TRAIN)\n",
    "\n",
    "    def loadData(self, dataType=DATASET.TRAIN):\n",
    "        data = MyDataSet(\n",
    "            seqLen=self.seqLen,\n",
    "            imageClassDir=imageClassDir,\n",
    "            imageVectorDir=imageVectorDir,\n",
    "            textDir=dataPrefix,\n",
    "            wordVocabDir=wordsPrefix,\n",
    "            dataType=dataType\n",
    "        )\n",
    "        return DataLoader(dataset=data, batch_size=self.batchSize, shuffle=True, num_workers=8,\n",
    "                          pin_memory=True, prefetch_factor=2, persistent_workers=False)\n",
    "\n",
    "    def test(self, dataType=DATASET.TEST, num=2000):\n",
    "        if isinstance(self.net, torch.nn.Module):\n",
    "            self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            testData = self.loadData(dataType=dataType)\n",
    "            count = 0\n",
    "            yPred, yTrue = [], []\n",
    "            for X, y, in testData:\n",
    "                self.XExample = X\n",
    "                if self.device == \"gpu\":\n",
    "                    y = y.cuda()\n",
    "                y_pred = self.net(X)\n",
    "                count += y_pred.shape[0]\n",
    "                if (dataType == DATASET.TRAIN) and (count > num):\n",
    "                    break\n",
    "                yPred.append(y_pred)\n",
    "                yTrue.append(y)\n",
    "        yPred = torch.cat(yPred, dim=0)\n",
    "        yTrue = torch.cat(yTrue, dim=0)\n",
    "        return getScore(y_pred=yPred.to(torch.device(\"cpu\")), y_true=yTrue.to(torch.device(\"cpu\")))\n",
    "\n",
    "    def train_epoch(self):\n",
    "        if isinstance(self.net, torch.nn.Module):\n",
    "            self.net.train()\n",
    "        if not isinstance(self.updater, torch.optim.Optimizer):\n",
    "            raise AttributeError\n",
    "        count = 0\n",
    "        for X, y in self.train_iter:\n",
    "            count += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            if self.device == \"gpu\":\n",
    "                y = y.cuda()\n",
    "            y_hat = self.net(X)\n",
    "            self.lrRecord.append(self.updater.state_dict()['param_groups'][0]['lr'])\n",
    "            l = self.loss(y_hat.squeeze(), y.squeeze()).mean()\n",
    "            self.updater.zero_grad()\n",
    "            l.backward()\n",
    "            nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=self.maxClipping, norm_type=self.normType)\n",
    "            # self.lr_scheduler.step(l)\n",
    "            self.updater.step()\n",
    "            self.lr_scheduler.step()\n",
    "            del X, y\n",
    "            # if count == 10: # 提前中止，测试用\n",
    "            #     break\n",
    "        gc.collect()\n",
    "\n",
    "    def train(self):\n",
    "        if self.device == \"gpu\":\n",
    "            self.net.to(device=try_gpu())\n",
    "        maxF1 = 0  # 以F1score为指标\n",
    "        patience = self.maxPatience  # 当前的容忍度\n",
    "        _, testScores, validScores, validStr = None, None, None, None\n",
    "        start = time.time()\n",
    "        for epoch in range(self.beforeEpoch, self.beforeEpoch + self.maxEpoch):\n",
    "            self.train_epoch()\n",
    "            if epoch % self.displayStep == 0:\n",
    "                # acc, pre, rec, f1, auc, loss # 元组内的顺序\n",
    "                trainScores, testScores, validScores = self.test(DATASET.TRAIN), self.test(DATASET.TEST), self.test(\n",
    "                    DATASET.VALID)\n",
    "                self.representationScores[epoch // self.displayStep] = tuple(\n",
    "                    zip(trainScores, testScores, validScores))  #\n",
    "                end = time.time()\n",
    "                print(\"----epoch:\", epoch, \"total cost:{:.2f} min\".format((end - start) / 60), \"---------\")\n",
    "                print(\"train, patience={}, acc:{:.3f}, pre:{:.3f}, rec:{:.3f}, f1:{:.3f}, acu:{:.3f},\"\n",
    "                      \"loss:{:.2f}\".format(patience, *trainScores))\n",
    "                print(\"test, patience={}, acc:{:.3f}, pre:{:.3f}, rec:{:.3f}, f1:{:.3f}, acu:{:.3f},\"\n",
    "                      \"loss:{:.2f}\".format(patience, *testScores))\n",
    "                validStr = \"valid, patience={}, acc:{:.3f}, pre:{:.3f}, rec:{:.3f}, f1:{:.3f}, acu:{:.3f}, loss:{:.4f}\".format(patience, *validScores)\n",
    "                print(validStr)\n",
    "                if testScores[3] > maxF1 + 1e-3:\n",
    "                    maxF1, patience = testScores[3], self.maxPatience\n",
    "                    self.saveNet(\"bestModel\", describe=validStr)\n",
    "                else:\n",
    "                    patience -= 1\n",
    "                if patience == 0:\n",
    "                    break\n",
    "        self.saveNet(describe=validStr)\n",
    "\n",
    "\n",
    "    def saveNet(self, saveName=time.strftime(\"%Y-%m-%d\", time.localtime()), describe=\"unKnown\"):\n",
    "        \"\"\"保存网络参数\"\"\"\n",
    "        savePath = saveModelWightsDir + saveName + \"/\"\n",
    "\n",
    "        if os.path.exists(savePath):\n",
    "            shutil.rmtree(savePath)  # 如果重新运行时，切忌如果有相同的文件名时要提前保存！！！！！\n",
    "        os.makedirs(savePath, exist_ok=True)\n",
    "        if not os.path.exists(savePath + \"logs/\"):\n",
    "            os.mkdir(savePath + \"logs/\")\n",
    "        if not os.path.exists(savePath + \"runs/\"):\n",
    "            os.mkdir(savePath + \"runs/\")\n",
    "\n",
    "        torch.save(self.net.state_dict(), savePath + saveName + \".pth\")\n",
    "\n",
    "        summaryWriter = SummaryWriter(log_dir=savePath + \"runs/\")\n",
    "        modelScoresVision(summaryWriter, scoresValues=self.representationScores, scoresNames=self.scoreNames,lrValues=self.lrRecord)\n",
    "        summaryWriter.close()\n",
    "\n",
    "        runLogs = (self.representationScores, self.lrRecord)\n",
    "        with open(savePath + \"logs/\" + saveName, 'wb+') as f:\n",
    "            pickle.dump(runLogs, f)\n",
    "\n",
    "        with open(savePath + \"describe.txt\", 'w+') as f:\n",
    "            f.write(\"acc, pre, rec, f1, auc, loss\\n\")\n",
    "            f.write(describe)\n",
    "\n",
    "    def loadNet(self, loadName=time.strftime(\"%Y-%m-%d\", time.localtime()), isEval=False):\n",
    "        \"\"\"加载网络参数\"\"\"\n",
    "\n",
    "        loadPath = saveModelWightsDir + loadName + \"/\"\n",
    "\n",
    "        self.net.load_state_dict(torch.load(loadPath + loadName + \".pth\"))\n",
    "\n",
    "        with open(loadPath + \"logs/\" + loadName, 'rb') as f:\n",
    "            self.representationScores, self.lrRecord = pickle.load(f)\n",
    "        self.beforeEpoch = len(self.representationScores)\n",
    "\n",
    "        if isEval:\n",
    "            self.net.eval()  # 不启用 BatchNormalization 和 Dropout\n",
    "\n",
    "print(\"Over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85931f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:47:00.525770Z",
     "iopub.status.busy": "2023-05-25T06:47:00.523901Z",
     "iopub.status.idle": "2023-05-25T11:24:34.143344Z",
     "shell.execute_reply": "2023-05-25T11:24:34.142072Z"
    },
    "papermill": {
     "duration": 16653.630904,
     "end_time": "2023-05-25T11:24:34.146767",
     "exception": false,
     "start_time": "2023-05-25T06:47:00.515863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/modelwights/bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----epoch: 0 total cost:3.15 min ---------\n",
      "train, patience=40, acc:0.626, pre:0.632, rec:0.387, f1:0.480, acu:0.659,loss:0.65\n",
      "test, patience=40, acc:0.634, pre:0.560, rec:0.373, f1:0.448, acu:0.639,loss:0.65\n",
      "valid, patience=40, acc:0.611, pre:0.518, rec:0.335, f1:0.407, acu:0.601, loss:0.6629\n",
      "----epoch: 1 total cost:6.45 min ---------\n",
      "train, patience=40, acc:0.671, pre:0.663, rec:0.470, f1:0.550, acu:0.742,loss:0.60\n",
      "test, patience=40, acc:0.687, pre:0.644, rec:0.474, f1:0.547, acu:0.748,loss:0.59\n",
      "valid, patience=40, acc:0.658, pre:0.599, rec:0.422, f1:0.495, acu:0.724, loss:0.6070\n",
      "----epoch: 2 total cost:9.57 min ---------\n",
      "train, patience=40, acc:0.842, pre:0.782, rec:0.876, f1:0.827, acu:0.905,loss:0.39\n",
      "test, patience=40, acc:0.798, pre:0.710, rec:0.832, f1:0.766, acu:0.864,loss:0.45\n",
      "valid, patience=40, acc:0.813, pre:0.727, rec:0.849, f1:0.783, acu:0.873, loss:0.4283\n",
      "----epoch: 3 total cost:12.71 min ---------\n",
      "train, patience=40, acc:0.845, pre:0.768, rec:0.932, f1:0.842, acu:0.912,loss:0.37\n",
      "test, patience=40, acc:0.788, pre:0.677, rec:0.893, f1:0.770, acu:0.864,loss:0.45\n",
      "valid, patience=40, acc:0.811, pre:0.703, rec:0.909, f1:0.793, acu:0.871, loss:0.4304\n",
      "----epoch: 4 total cost:15.98 min ---------\n",
      "train, patience=40, acc:0.853, pre:0.782, rec:0.918, f1:0.844, acu:0.923,loss:0.36\n",
      "test, patience=40, acc:0.812, pre:0.714, rec:0.880, f1:0.788, acu:0.877,loss:0.45\n",
      "valid, patience=40, acc:0.831, pre:0.736, rec:0.897, f1:0.808, acu:0.882, loss:0.4315\n",
      "----epoch: 5 total cost:19.15 min ---------\n",
      "train, patience=40, acc:0.877, pre:0.870, rec:0.857, f1:0.864, acu:0.941,loss:0.34\n",
      "test, patience=40, acc:0.840, pre:0.791, rec:0.812, f1:0.801, acu:0.895,loss:0.40\n",
      "valid, patience=40, acc:0.841, pre:0.781, rec:0.834, f1:0.807, acu:0.898, loss:0.3856\n",
      "----epoch: 6 total cost:22.69 min ---------\n",
      "train, patience=40, acc:0.888, pre:0.842, rec:0.922, f1:0.880, acu:0.941,loss:0.31\n",
      "test, patience=40, acc:0.841, pre:0.756, rec:0.888, f1:0.817, acu:0.900,loss:0.39\n",
      "valid, patience=40, acc:0.852, pre:0.766, rec:0.905, f1:0.830, acu:0.900, loss:0.3828\n",
      "----epoch: 7 total cost:26.16 min ---------\n",
      "train, patience=40, acc:0.880, pre:0.822, rec:0.922, f1:0.869, acu:0.940,loss:0.31\n",
      "test, patience=40, acc:0.845, pre:0.762, rec:0.889, f1:0.821, acu:0.901,loss:0.39\n",
      "valid, patience=40, acc:0.851, pre:0.767, rec:0.900, f1:0.828, acu:0.902, loss:0.3744\n",
      "----epoch: 8 total cost:29.90 min ---------\n",
      "train, patience=40, acc:0.898, pre:0.879, rec:0.891, f1:0.885, acu:0.951,loss:0.28\n",
      "test, patience=40, acc:0.856, pre:0.809, rec:0.834, f1:0.821, acu:0.909,loss:0.38\n",
      "valid, patience=40, acc:0.863, pre:0.815, rec:0.848, f1:0.831, acu:0.910, loss:0.3719\n",
      "----epoch: 9 total cost:33.17 min ---------\n",
      "train, patience=39, acc:0.902, pre:0.861, rec:0.921, f1:0.890, acu:0.949,loss:0.28\n",
      "test, patience=39, acc:0.860, pre:0.779, rec:0.905, f1:0.837, acu:0.908,loss:0.36\n",
      "valid, patience=39, acc:0.861, pre:0.779, rec:0.910, f1:0.839, acu:0.911, loss:0.3496\n",
      "----epoch: 10 total cost:36.65 min ---------\n",
      "train, patience=40, acc:0.915, pre:0.892, rec:0.909, f1:0.900, acu:0.958,loss:0.25\n",
      "test, patience=40, acc:0.873, pre:0.817, rec:0.877, f1:0.846, acu:0.915,loss:0.36\n",
      "valid, patience=40, acc:0.881, pre:0.828, rec:0.884, f1:0.855, acu:0.921, loss:0.3375\n",
      "----epoch: 11 total cost:40.17 min ---------\n",
      "train, patience=40, acc:0.924, pre:0.885, rec:0.946, f1:0.914, acu:0.963,loss:0.24\n",
      "test, patience=40, acc:0.873, pre:0.800, rec:0.907, f1:0.850, acu:0.916,loss:0.35\n",
      "valid, patience=40, acc:0.880, pre:0.802, rec:0.927, f1:0.860, acu:0.924, loss:0.3264\n",
      "----epoch: 12 total cost:43.85 min ---------\n",
      "train, patience=40, acc:0.919, pre:0.894, rec:0.926, f1:0.910, acu:0.964,loss:0.23\n",
      "test, patience=40, acc:0.876, pre:0.816, rec:0.889, f1:0.851, acu:0.918,loss:0.35\n",
      "valid, patience=40, acc:0.887, pre:0.829, rec:0.901, f1:0.864, acu:0.929, loss:0.3190\n",
      "----epoch: 13 total cost:47.37 min ---------\n",
      "train, patience=39, acc:0.934, pre:0.902, rec:0.954, f1:0.928, acu:0.970,loss:0.21\n",
      "test, patience=39, acc:0.873, pre:0.803, rec:0.903, f1:0.850, acu:0.919,loss:0.36\n",
      "valid, patience=39, acc:0.883, pre:0.813, rec:0.919, f1:0.862, acu:0.930, loss:0.3250\n",
      "----epoch: 14 total cost:51.16 min ---------\n",
      "train, patience=38, acc:0.933, pre:0.909, rec:0.936, f1:0.922, acu:0.972,loss:0.21\n",
      "test, patience=38, acc:0.881, pre:0.823, rec:0.894, f1:0.857, acu:0.921,loss:0.34\n",
      "valid, patience=38, acc:0.889, pre:0.833, rec:0.902, f1:0.866, acu:0.933, loss:0.3103\n",
      "----epoch: 15 total cost:54.57 min ---------\n",
      "train, patience=40, acc:0.930, pre:0.912, rec:0.933, f1:0.922, acu:0.970,loss:0.21\n",
      "test, patience=40, acc:0.880, pre:0.821, rec:0.895, f1:0.856, acu:0.921,loss:0.34\n",
      "valid, patience=40, acc:0.889, pre:0.833, rec:0.902, f1:0.866, acu:0.932, loss:0.3111\n",
      "----epoch: 16 total cost:58.04 min ---------\n",
      "train, patience=39, acc:0.929, pre:0.943, rec:0.894, f1:0.918, acu:0.970,loss:0.22\n",
      "test, patience=39, acc:0.881, pre:0.845, rec:0.858, f1:0.852, acu:0.924,loss:0.34\n",
      "valid, patience=39, acc:0.897, pre:0.867, rec:0.875, f1:0.871, acu:0.936, loss:0.3010\n",
      "----epoch: 17 total cost:61.52 min ---------\n",
      "train, patience=38, acc:0.923, pre:0.893, rec:0.940, f1:0.916, acu:0.975,loss:0.21\n",
      "test, patience=38, acc:0.880, pre:0.813, rec:0.905, f1:0.857, acu:0.926,loss:0.38\n",
      "valid, patience=38, acc:0.890, pre:0.826, rec:0.916, f1:0.868, acu:0.936, loss:0.3449\n",
      "----epoch: 18 total cost:65.22 min ---------\n",
      "train, patience=37, acc:0.939, pre:0.912, rec:0.951, f1:0.931, acu:0.979,loss:0.19\n",
      "test, patience=37, acc:0.877, pre:0.806, rec:0.908, f1:0.854, acu:0.924,loss:0.34\n",
      "valid, patience=37, acc:0.890, pre:0.821, rec:0.924, f1:0.869, acu:0.942, loss:0.2981\n",
      "----epoch: 19 total cost:68.82 min ---------\n",
      "train, patience=36, acc:0.914, pre:0.852, rec:0.964, f1:0.905, acu:0.977,loss:0.23\n",
      "test, patience=36, acc:0.867, pre:0.782, rec:0.924, f1:0.847, acu:0.929,loss:0.41\n",
      "valid, patience=36, acc:0.877, pre:0.790, rec:0.941, f1:0.859, acu:0.944, loss:0.3553\n",
      "----epoch: 20 total cost:72.23 min ---------\n",
      "train, patience=35, acc:0.935, pre:0.926, rec:0.928, f1:0.927, acu:0.982,loss:0.17\n",
      "test, patience=35, acc:0.885, pre:0.828, rec:0.899, f1:0.862, acu:0.931,loss:0.35\n",
      "valid, patience=35, acc:0.902, pre:0.851, rec:0.914, f1:0.881, acu:0.948, loss:0.2917\n",
      "----epoch: 21 total cost:75.84 min ---------\n",
      "train, patience=40, acc:0.931, pre:0.899, rec:0.948, f1:0.923, acu:0.983,loss:0.18\n",
      "test, patience=40, acc:0.878, pre:0.806, rec:0.914, f1:0.857, acu:0.932,loss:0.39\n",
      "valid, patience=40, acc:0.894, pre:0.823, rec:0.933, f1:0.875, acu:0.949, loss:0.3244\n",
      "----epoch: 22 total cost:79.54 min ---------\n",
      "train, patience=39, acc:0.951, pre:0.953, rec:0.935, f1:0.944, acu:0.984,loss:0.15\n",
      "test, patience=39, acc:0.892, pre:0.846, rec:0.889, f1:0.867, acu:0.933,loss:0.35\n",
      "valid, patience=39, acc:0.904, pre:0.867, rec:0.896, f1:0.881, acu:0.949, loss:0.2820\n",
      "----epoch: 23 total cost:83.06 min ---------\n",
      "train, patience=40, acc:0.945, pre:0.938, rec:0.931, f1:0.935, acu:0.983,loss:0.16\n",
      "test, patience=40, acc:0.888, pre:0.838, rec:0.893, f1:0.864, acu:0.933,loss:0.35\n",
      "valid, patience=40, acc:0.903, pre:0.862, rec:0.901, f1:0.881, acu:0.952, loss:0.2805\n",
      "----epoch: 24 total cost:86.57 min ---------\n",
      "train, patience=39, acc:0.944, pre:0.951, rec:0.918, f1:0.934, acu:0.984,loss:0.16\n",
      "test, patience=39, acc:0.890, pre:0.849, rec:0.881, f1:0.865, acu:0.933,loss:0.35\n",
      "valid, patience=39, acc:0.904, pre:0.878, rec:0.882, f1:0.880, acu:0.952, loss:0.2780\n",
      "----epoch: 25 total cost:90.06 min ---------\n",
      "train, patience=38, acc:0.944, pre:0.921, rec:0.946, f1:0.933, acu:0.986,loss:0.15\n",
      "test, patience=38, acc:0.882, pre:0.820, rec:0.900, f1:0.858, acu:0.934,loss:0.38\n",
      "valid, patience=38, acc:0.902, pre:0.847, rec:0.922, f1:0.883, acu:0.952, loss:0.2966\n",
      "----epoch: 26 total cost:93.53 min ---------\n",
      "train, patience=37, acc:0.938, pre:0.913, rec:0.952, f1:0.932, acu:0.984,loss:0.16\n",
      "test, patience=37, acc:0.883, pre:0.819, rec:0.907, f1:0.861, acu:0.934,loss:0.38\n",
      "valid, patience=37, acc:0.901, pre:0.841, rec:0.926, f1:0.881, acu:0.952, loss:0.2963\n",
      "----epoch: 27 total cost:97.29 min ---------\n",
      "train, patience=36, acc:0.943, pre:0.925, rec:0.950, f1:0.937, acu:0.987,loss:0.15\n",
      "test, patience=36, acc:0.885, pre:0.827, rec:0.899, f1:0.862, acu:0.934,loss:0.37\n",
      "valid, patience=36, acc:0.904, pre:0.853, rec:0.917, f1:0.883, acu:0.953, loss:0.2878\n",
      "----epoch: 28 total cost:100.72 min ---------\n",
      "train, patience=35, acc:0.952, pre:0.917, rec:0.971, f1:0.943, acu:0.988,loss:0.14\n",
      "test, patience=35, acc:0.885, pre:0.822, rec:0.908, f1:0.863, acu:0.934,loss:0.38\n",
      "valid, patience=35, acc:0.904, pre:0.846, rec:0.927, f1:0.885, acu:0.953, loss:0.2946\n",
      "----epoch: 29 total cost:104.24 min ---------\n",
      "train, patience=34, acc:0.949, pre:0.922, rec:0.964, f1:0.943, acu:0.988,loss:0.15\n",
      "test, patience=34, acc:0.885, pre:0.823, rec:0.904, f1:0.862, acu:0.934,loss:0.38\n",
      "valid, patience=34, acc:0.905, pre:0.849, rec:0.924, f1:0.885, acu:0.953, loss:0.2947\n",
      "----epoch: 30 total cost:107.71 min ---------\n",
      "train, patience=33, acc:0.952, pre:0.931, rec:0.960, f1:0.946, acu:0.988,loss:0.14\n",
      "test, patience=33, acc:0.884, pre:0.820, rec:0.908, f1:0.862, acu:0.934,loss:0.39\n",
      "valid, patience=33, acc:0.904, pre:0.845, rec:0.929, f1:0.885, acu:0.953, loss:0.2986\n",
      "----epoch: 31 total cost:111.38 min ---------\n",
      "train, patience=32, acc:0.953, pre:0.925, rec:0.969, f1:0.946, acu:0.990,loss:0.13\n",
      "test, patience=32, acc:0.884, pre:0.819, rec:0.909, f1:0.862, acu:0.934,loss:0.39\n",
      "valid, patience=32, acc:0.903, pre:0.844, rec:0.929, f1:0.884, acu:0.953, loss:0.2995\n",
      "----epoch: 32 total cost:115.01 min ---------\n",
      "train, patience=31, acc:0.946, pre:0.925, rec:0.955, f1:0.940, acu:0.986,loss:0.15\n",
      "test, patience=31, acc:0.885, pre:0.822, rec:0.907, f1:0.863, acu:0.934,loss:0.38\n",
      "valid, patience=31, acc:0.904, pre:0.847, rec:0.927, f1:0.885, acu:0.953, loss:0.2964\n",
      "----epoch: 33 total cost:118.48 min ---------\n",
      "train, patience=30, acc:0.947, pre:0.924, rec:0.961, f1:0.942, acu:0.986,loss:0.15\n",
      "test, patience=30, acc:0.887, pre:0.829, rec:0.903, f1:0.864, acu:0.934,loss:0.39\n",
      "valid, patience=30, acc:0.906, pre:0.860, rec:0.911, f1:0.885, acu:0.953, loss:0.2994\n",
      "----epoch: 34 total cost:122.02 min ---------\n",
      "train, patience=29, acc:0.952, pre:0.938, rec:0.955, f1:0.946, acu:0.989,loss:0.13\n",
      "test, patience=29, acc:0.891, pre:0.833, rec:0.908, f1:0.869, acu:0.934,loss:0.38\n",
      "valid, patience=29, acc:0.909, pre:0.856, rec:0.927, f1:0.890, acu:0.954, loss:0.2859\n",
      "----epoch: 35 total cost:125.50 min ---------\n",
      "train, patience=40, acc:0.946, pre:0.913, rec:0.969, f1:0.940, acu:0.989,loss:0.15\n",
      "test, patience=40, acc:0.884, pre:0.813, rec:0.919, f1:0.863, acu:0.935,loss:0.40\n",
      "valid, patience=40, acc:0.905, pre:0.838, rec:0.942, f1:0.887, acu:0.955, loss:0.3055\n",
      "----epoch: 36 total cost:129.16 min ---------\n",
      "train, patience=39, acc:0.952, pre:0.946, rec:0.941, f1:0.944, acu:0.990,loss:0.13\n",
      "test, patience=39, acc:0.893, pre:0.849, rec:0.889, f1:0.869, acu:0.934,loss:0.38\n",
      "valid, patience=39, acc:0.912, pre:0.883, rec:0.899, f1:0.891, acu:0.956, loss:0.2811\n",
      "----epoch: 37 total cost:132.75 min ---------\n",
      "train, patience=38, acc:0.960, pre:0.945, rec:0.961, f1:0.953, acu:0.993,loss:0.11\n",
      "test, patience=38, acc:0.896, pre:0.849, rec:0.899, f1:0.873, acu:0.936,loss:0.39\n",
      "valid, patience=38, acc:0.913, pre:0.876, rec:0.910, f1:0.893, acu:0.955, loss:0.2928\n",
      "----epoch: 38 total cost:136.26 min ---------\n",
      "train, patience=40, acc:0.955, pre:0.918, rec:0.982, f1:0.949, acu:0.994,loss:0.11\n",
      "test, patience=40, acc:0.888, pre:0.819, rec:0.922, f1:0.868, acu:0.934,loss:0.41\n",
      "valid, patience=40, acc:0.907, pre:0.842, rec:0.943, f1:0.890, acu:0.956, loss:0.3026\n",
      "----epoch: 39 total cost:139.72 min ---------\n",
      "train, patience=39, acc:0.958, pre:0.928, rec:0.978, f1:0.953, acu:0.993,loss:0.11\n",
      "test, patience=39, acc:0.891, pre:0.828, rec:0.918, f1:0.870, acu:0.935,loss:0.41\n",
      "valid, patience=39, acc:0.909, pre:0.848, rec:0.940, f1:0.891, acu:0.955, loss:0.3078\n",
      "----epoch: 40 total cost:143.31 min ---------\n",
      "train, patience=38, acc:0.961, pre:0.953, rec:0.958, f1:0.956, acu:0.992,loss:0.11\n",
      "test, patience=38, acc:0.897, pre:0.843, rec:0.909, f1:0.875, acu:0.936,loss:0.40\n",
      "valid, patience=38, acc:0.917, pre:0.873, rec:0.926, f1:0.899, acu:0.957, loss:0.2864\n",
      "----epoch: 41 total cost:146.98 min ---------\n",
      "train, patience=40, acc:0.966, pre:0.946, rec:0.976, f1:0.961, acu:0.993,loss:0.10\n",
      "test, patience=40, acc:0.895, pre:0.838, rec:0.913, f1:0.874, acu:0.935,loss:0.41\n",
      "valid, patience=40, acc:0.915, pre:0.865, rec:0.932, f1:0.898, acu:0.957, loss:0.2961\n",
      "----epoch: 42 total cost:150.46 min ---------\n",
      "train, patience=39, acc:0.961, pre:0.941, rec:0.974, f1:0.958, acu:0.992,loss:0.11\n",
      "test, patience=39, acc:0.892, pre:0.833, rec:0.911, f1:0.871, acu:0.936,loss:0.43\n",
      "valid, patience=39, acc:0.911, pre:0.856, rec:0.933, f1:0.893, acu:0.957, loss:0.3126\n",
      "----epoch: 43 total cost:153.90 min ---------\n",
      "train, patience=38, acc:0.963, pre:0.959, rec:0.956, f1:0.958, acu:0.993,loss:0.10\n",
      "test, patience=38, acc:0.895, pre:0.852, rec:0.892, f1:0.871, acu:0.935,loss:0.43\n",
      "valid, patience=38, acc:0.916, pre:0.887, rec:0.904, f1:0.896, acu:0.957, loss:0.3009\n",
      "----epoch: 44 total cost:157.24 min ---------\n",
      "train, patience=37, acc:0.973, pre:0.959, rec:0.979, f1:0.969, acu:0.995,loss:0.09\n",
      "test, patience=37, acc:0.891, pre:0.830, rec:0.914, f1:0.870, acu:0.935,loss:0.42\n",
      "valid, patience=37, acc:0.913, pre:0.857, rec:0.937, f1:0.895, acu:0.957, loss:0.3001\n",
      "----epoch: 45 total cost:160.56 min ---------\n",
      "train, patience=36, acc:0.965, pre:0.936, rec:0.988, f1:0.961, acu:0.995,loss:0.10\n",
      "test, patience=36, acc:0.886, pre:0.819, rec:0.917, f1:0.865, acu:0.934,loss:0.44\n",
      "valid, patience=36, acc:0.910, pre:0.848, rec:0.943, f1:0.893, acu:0.957, loss:0.3158\n",
      "----epoch: 46 total cost:163.99 min ---------\n",
      "train, patience=35, acc:0.976, pre:0.978, rec:0.968, f1:0.973, acu:0.996,loss:0.08\n",
      "test, patience=35, acc:0.895, pre:0.846, rec:0.901, f1:0.873, acu:0.936,loss:0.42\n",
      "valid, patience=35, acc:0.917, pre:0.878, rec:0.919, f1:0.898, acu:0.957, loss:0.2951\n",
      "----epoch: 47 total cost:167.35 min ---------\n",
      "train, patience=34, acc:0.980, pre:0.972, rec:0.982, f1:0.977, acu:0.996,loss:0.07\n",
      "test, patience=34, acc:0.895, pre:0.842, rec:0.908, f1:0.874, acu:0.934,loss:0.43\n",
      "valid, patience=34, acc:0.918, pre:0.877, rec:0.924, f1:0.900, acu:0.957, loss:0.2985\n",
      "----epoch: 48 total cost:170.68 min ---------\n",
      "train, patience=33, acc:0.973, pre:0.965, rec:0.974, f1:0.969, acu:0.995,loss:0.08\n",
      "test, patience=33, acc:0.893, pre:0.839, rec:0.905, f1:0.871, acu:0.936,loss:0.43\n",
      "valid, patience=33, acc:0.917, pre:0.871, rec:0.928, f1:0.899, acu:0.957, loss:0.3043\n",
      "----epoch: 49 total cost:174.23 min ---------\n",
      "train, patience=32, acc:0.972, pre:0.956, rec:0.980, f1:0.968, acu:0.996,loss:0.08\n",
      "test, patience=32, acc:0.894, pre:0.840, rec:0.906, f1:0.872, acu:0.935,loss:0.43\n",
      "valid, patience=32, acc:0.914, pre:0.869, rec:0.924, f1:0.895, acu:0.957, loss:0.3046\n",
      "----epoch: 50 total cost:177.61 min ---------\n",
      "train, patience=31, acc:0.976, pre:0.964, rec:0.981, f1:0.972, acu:0.996,loss:0.08\n",
      "test, patience=31, acc:0.892, pre:0.835, rec:0.909, f1:0.871, acu:0.935,loss:0.43\n",
      "valid, patience=31, acc:0.916, pre:0.869, rec:0.928, f1:0.898, acu:0.958, loss:0.2998\n",
      "----epoch: 51 total cost:180.84 min ---------\n",
      "train, patience=30, acc:0.966, pre:0.977, rec:0.945, f1:0.961, acu:0.995,loss:0.09\n",
      "test, patience=30, acc:0.897, pre:0.856, rec:0.891, f1:0.873, acu:0.935,loss:0.42\n",
      "valid, patience=30, acc:0.916, pre:0.890, rec:0.899, f1:0.895, acu:0.957, loss:0.3002\n",
      "----epoch: 52 total cost:184.11 min ---------\n",
      "train, patience=29, acc:0.972, pre:0.954, rec:0.982, f1:0.968, acu:0.997,loss:0.08\n",
      "test, patience=29, acc:0.890, pre:0.828, rec:0.913, f1:0.869, acu:0.935,loss:0.46\n",
      "valid, patience=29, acc:0.915, pre:0.861, rec:0.936, f1:0.897, acu:0.957, loss:0.3239\n",
      "----epoch: 53 total cost:187.37 min ---------\n",
      "train, patience=28, acc:0.971, pre:0.963, rec:0.969, f1:0.966, acu:0.995,loss:0.08\n",
      "test, patience=28, acc:0.896, pre:0.846, rec:0.903, f1:0.873, acu:0.935,loss:0.45\n",
      "valid, patience=28, acc:0.916, pre:0.874, rec:0.922, f1:0.897, acu:0.957, loss:0.3116\n",
      "----epoch: 54 total cost:190.59 min ---------\n",
      "train, patience=27, acc:0.978, pre:0.970, rec:0.980, f1:0.975, acu:0.996,loss:0.07\n",
      "test, patience=27, acc:0.895, pre:0.841, rec:0.908, f1:0.873, acu:0.935,loss:0.46\n",
      "valid, patience=27, acc:0.917, pre:0.870, rec:0.932, f1:0.900, acu:0.957, loss:0.3213\n",
      "----epoch: 55 total cost:193.90 min ---------\n",
      "train, patience=26, acc:0.977, pre:0.970, rec:0.975, f1:0.973, acu:0.997,loss:0.07\n",
      "test, patience=26, acc:0.896, pre:0.848, rec:0.899, f1:0.873, acu:0.935,loss:0.44\n",
      "valid, patience=26, acc:0.917, pre:0.880, rec:0.916, f1:0.897, acu:0.957, loss:0.3051\n",
      "----epoch: 56 total cost:197.32 min ---------\n",
      "train, patience=25, acc:0.976, pre:0.978, rec:0.967, f1:0.973, acu:0.996,loss:0.08\n",
      "test, patience=25, acc:0.897, pre:0.851, rec:0.897, f1:0.874, acu:0.935,loss:0.45\n",
      "valid, patience=25, acc:0.915, pre:0.882, rec:0.907, f1:0.895, acu:0.957, loss:0.3148\n",
      "----epoch: 57 total cost:200.55 min ---------\n",
      "train, patience=24, acc:0.974, pre:0.964, rec:0.977, f1:0.970, acu:0.996,loss:0.07\n",
      "test, patience=24, acc:0.894, pre:0.840, rec:0.906, f1:0.872, acu:0.935,loss:0.45\n",
      "valid, patience=24, acc:0.917, pre:0.872, rec:0.927, f1:0.898, acu:0.957, loss:0.3125\n",
      "----epoch: 58 total cost:203.92 min ---------\n",
      "train, patience=23, acc:0.979, pre:0.977, rec:0.977, f1:0.977, acu:0.997,loss:0.06\n",
      "test, patience=23, acc:0.896, pre:0.846, rec:0.903, f1:0.873, acu:0.935,loss:0.45\n",
      "valid, patience=23, acc:0.916, pre:0.874, rec:0.921, f1:0.897, acu:0.957, loss:0.3113\n",
      "----epoch: 59 total cost:207.29 min ---------\n",
      "train, patience=22, acc:0.975, pre:0.965, rec:0.977, f1:0.971, acu:0.995,loss:0.08\n",
      "test, patience=22, acc:0.895, pre:0.842, rec:0.905, f1:0.872, acu:0.935,loss:0.45\n",
      "valid, patience=22, acc:0.917, pre:0.873, rec:0.926, f1:0.899, acu:0.957, loss:0.3162\n",
      "----epoch: 60 total cost:210.60 min ---------\n",
      "train, patience=21, acc:0.981, pre:0.983, rec:0.974, f1:0.979, acu:0.997,loss:0.07\n",
      "test, patience=21, acc:0.895, pre:0.845, rec:0.903, f1:0.873, acu:0.935,loss:0.45\n",
      "valid, patience=21, acc:0.915, pre:0.873, rec:0.921, f1:0.896, acu:0.957, loss:0.3146\n",
      "----epoch: 61 total cost:213.95 min ---------\n",
      "train, patience=20, acc:0.973, pre:0.958, rec:0.985, f1:0.971, acu:0.996,loss:0.08\n",
      "test, patience=20, acc:0.893, pre:0.837, rec:0.908, f1:0.871, acu:0.935,loss:0.46\n",
      "valid, patience=20, acc:0.917, pre:0.867, rec:0.934, f1:0.900, acu:0.957, loss:0.3227\n",
      "----epoch: 62 total cost:217.19 min ---------\n",
      "train, patience=19, acc:0.980, pre:0.971, rec:0.983, f1:0.977, acu:0.996,loss:0.07\n",
      "test, patience=19, acc:0.893, pre:0.837, rec:0.907, f1:0.871, acu:0.935,loss:0.46\n",
      "valid, patience=19, acc:0.917, pre:0.868, rec:0.934, f1:0.900, acu:0.957, loss:0.3206\n",
      "----epoch: 63 total cost:220.50 min ---------\n",
      "train, patience=18, acc:0.975, pre:0.970, rec:0.973, f1:0.972, acu:0.997,loss:0.07\n",
      "test, patience=18, acc:0.893, pre:0.838, rec:0.907, f1:0.871, acu:0.935,loss:0.46\n",
      "valid, patience=18, acc:0.918, pre:0.871, rec:0.932, f1:0.901, acu:0.957, loss:0.3188\n",
      "----epoch: 64 total cost:223.87 min ---------\n",
      "train, patience=17, acc:0.984, pre:0.978, rec:0.987, f1:0.982, acu:0.997,loss:0.06\n",
      "test, patience=17, acc:0.893, pre:0.839, rec:0.905, f1:0.871, acu:0.935,loss:0.45\n",
      "valid, patience=17, acc:0.916, pre:0.871, rec:0.926, f1:0.898, acu:0.957, loss:0.3160\n",
      "----epoch: 65 total cost:227.25 min ---------\n",
      "train, patience=16, acc:0.982, pre:0.970, rec:0.988, f1:0.979, acu:0.997,loss:0.07\n",
      "test, patience=16, acc:0.893, pre:0.839, rec:0.905, f1:0.871, acu:0.935,loss:0.46\n",
      "valid, patience=16, acc:0.916, pre:0.871, rec:0.926, f1:0.898, acu:0.957, loss:0.3163\n",
      "----epoch: 66 total cost:230.63 min ---------\n",
      "train, patience=15, acc:0.971, pre:0.943, rec:0.992, f1:0.967, acu:0.996,loss:0.08\n",
      "test, patience=15, acc:0.890, pre:0.829, rec:0.911, f1:0.868, acu:0.935,loss:0.46\n",
      "valid, patience=15, acc:0.912, pre:0.855, rec:0.937, f1:0.894, acu:0.956, loss:0.3297\n",
      "----epoch: 67 total cost:234.12 min ---------\n",
      "train, patience=14, acc:0.978, pre:0.969, rec:0.982, f1:0.975, acu:0.998,loss:0.06\n",
      "test, patience=14, acc:0.895, pre:0.843, rec:0.905, f1:0.873, acu:0.935,loss:0.43\n",
      "valid, patience=14, acc:0.913, pre:0.868, rec:0.923, f1:0.894, acu:0.956, loss:0.3110\n",
      "----epoch: 68 total cost:237.39 min ---------\n",
      "train, patience=13, acc:0.971, pre:0.947, rec:0.992, f1:0.969, acu:0.996,loss:0.08\n",
      "test, patience=13, acc:0.888, pre:0.824, rec:0.913, f1:0.866, acu:0.934,loss:0.48\n",
      "valid, patience=13, acc:0.907, pre:0.843, rec:0.941, f1:0.889, acu:0.956, loss:0.3433\n",
      "----epoch: 69 total cost:240.61 min ---------\n",
      "train, patience=12, acc:0.982, pre:0.984, rec:0.975, f1:0.979, acu:0.998,loss:0.06\n",
      "test, patience=12, acc:0.896, pre:0.851, rec:0.896, f1:0.873, acu:0.934,loss:0.44\n",
      "valid, patience=12, acc:0.916, pre:0.880, rec:0.912, f1:0.896, acu:0.957, loss:0.3065\n",
      "----epoch: 70 total cost:244.15 min ---------\n",
      "train, patience=11, acc:0.973, pre:0.963, rec:0.976, f1:0.970, acu:0.996,loss:0.08\n",
      "test, patience=11, acc:0.896, pre:0.847, rec:0.903, f1:0.874, acu:0.936,loss:0.47\n",
      "valid, patience=11, acc:0.915, pre:0.869, rec:0.925, f1:0.896, acu:0.956, loss:0.3348\n",
      "----epoch: 71 total cost:247.44 min ---------\n",
      "train, patience=10, acc:0.982, pre:0.987, rec:0.970, f1:0.979, acu:0.998,loss:0.06\n",
      "test, patience=10, acc:0.897, pre:0.862, rec:0.883, f1:0.872, acu:0.934,loss:0.45\n",
      "valid, patience=10, acc:0.915, pre:0.893, rec:0.895, f1:0.894, acu:0.957, loss:0.3115\n",
      "----epoch: 72 total cost:250.93 min ---------\n",
      "train, patience=9, acc:0.973, pre:0.975, rec:0.964, f1:0.969, acu:0.996,loss:0.07\n",
      "test, patience=9, acc:0.895, pre:0.850, rec:0.895, f1:0.872, acu:0.935,loss:0.46\n",
      "valid, patience=9, acc:0.918, pre:0.885, rec:0.913, f1:0.899, acu:0.957, loss:0.3169\n",
      "----epoch: 73 total cost:254.18 min ---------\n",
      "train, patience=8, acc:0.973, pre:0.955, rec:0.987, f1:0.971, acu:0.997,loss:0.08\n",
      "test, patience=8, acc:0.896, pre:0.836, rec:0.919, f1:0.875, acu:0.934,loss:0.49\n",
      "valid, patience=8, acc:0.910, pre:0.853, rec:0.934, f1:0.892, acu:0.955, loss:0.3550\n",
      "----epoch: 74 total cost:257.61 min ---------\n",
      "train, patience=7, acc:0.980, pre:0.977, rec:0.977, f1:0.977, acu:0.997,loss:0.06\n",
      "test, patience=7, acc:0.895, pre:0.850, rec:0.893, f1:0.871, acu:0.935,loss:0.48\n",
      "valid, patience=7, acc:0.919, pre:0.882, rec:0.919, f1:0.900, acu:0.956, loss:0.3400\n",
      "----epoch: 75 total cost:260.81 min ---------\n",
      "train, patience=6, acc:0.986, pre:0.989, rec:0.979, f1:0.984, acu:0.999,loss:0.05\n",
      "test, patience=6, acc:0.895, pre:0.852, rec:0.893, f1:0.872, acu:0.934,loss:0.47\n",
      "valid, patience=6, acc:0.917, pre:0.884, rec:0.909, f1:0.897, acu:0.956, loss:0.3312\n",
      "----epoch: 76 total cost:264.03 min ---------\n",
      "train, patience=5, acc:0.981, pre:0.970, rec:0.987, f1:0.978, acu:0.998,loss:0.06\n",
      "test, patience=5, acc:0.895, pre:0.843, rec:0.904, f1:0.872, acu:0.935,loss:0.49\n",
      "valid, patience=5, acc:0.916, pre:0.871, rec:0.926, f1:0.897, acu:0.957, loss:0.3390\n",
      "----epoch: 77 total cost:267.33 min ---------\n",
      "train, patience=4, acc:0.969, pre:0.989, rec:0.940, f1:0.964, acu:0.997,loss:0.08\n",
      "test, patience=4, acc:0.890, pre:0.869, rec:0.854, f1:0.861, acu:0.934,loss:0.49\n",
      "valid, patience=4, acc:0.913, pre:0.907, rec:0.871, f1:0.888, acu:0.956, loss:0.3548\n",
      "----epoch: 78 total cost:270.74 min ---------\n",
      "train, patience=3, acc:0.982, pre:0.969, rec:0.993, f1:0.981, acu:0.999,loss:0.05\n",
      "test, patience=3, acc:0.890, pre:0.828, rec:0.913, f1:0.869, acu:0.934,loss:0.52\n",
      "valid, patience=3, acc:0.909, pre:0.847, rec:0.941, f1:0.891, acu:0.956, loss:0.3665\n",
      "----epoch: 79 total cost:273.93 min ---------\n",
      "train, patience=2, acc:0.983, pre:0.986, rec:0.975, f1:0.980, acu:0.998,loss:0.06\n",
      "test, patience=2, acc:0.897, pre:0.853, rec:0.895, f1:0.873, acu:0.935,loss:0.50\n",
      "valid, patience=2, acc:0.917, pre:0.884, rec:0.912, f1:0.898, acu:0.956, loss:0.3473\n",
      "----epoch: 80 total cost:277.27 min ---------\n",
      "train, patience=1, acc:0.983, pre:0.976, rec:0.985, f1:0.981, acu:0.998,loss:0.05\n",
      "test, patience=1, acc:0.894, pre:0.842, rec:0.904, f1:0.872, acu:0.935,loss:0.53\n",
      "valid, patience=1, acc:0.915, pre:0.867, rec:0.927, f1:0.896, acu:0.956, loss:0.3679\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "main = Main(\"gpu\")\n",
    "main.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16675.996885,
   "end_time": "2023-05-25T11:24:38.284881",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-25T06:46:42.287996",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
